---
title: "Дискриминантный анализ"
subtitle: "Математические методы в зоологии с использованием R"
author: "Марина Варфоломеева"
classoption: 't,xcolor=table'
language: russian, english
output:
  beamer_presentation:
    colortheme: seagull
    highlight: tango
    fonttheme: structurebold
    latex_engine: xelatex
    includes:
      in_header: ./includes/header.tex
    pandoc_args:
    - -V fontsize=10pt
    slide_level: 2
    fig_crop: false
    theme: CambridgeUS
    toc: no
---


```{r setup, include = FALSE, cache = FALSE, purl = FALSE}
options(width = 70, scipen = 6)
library(knitr)
opts_chunk$set(fig.show='hold', size='footnotesize', comment="#", warning=FALSE, message=FALSE, dev='cairo_pdf', fig.height=2.5, fig.width=7.7)
```

## Дискриминантный анализ

### Вы сможете

- провести линейный дискриминантный анализ с использованием обучающей выборки и проверить качество классификации на тестовых данных или с использованием кроссвалидации
- проверить условия применимости дискриминантного анализа

## Дискриминантный анализ


## Дискриминантный анализ

\blockbegin{Дискриминантный анализ}

- метод классификации объектов с учителем (__supervised learning__), т.е. применяется, когда принадлежность объектов к группе заранее известна.

\blockend

\vfill

Задачи дискриминантного анализа:

- выяснить, какие признаки лучше всего позволяют классифицировать объекты
- выяснить правило классификации существующих объектов
- классификация новых объектов неизвестной принадлежности по этому правилу



## Дискриминантный анализ

Нужно найти такую ось, вдоль которой группы различаются лучше всего, с минимальным перекрыванием.

Как она может проходить?

\includegraphics[width=0.8\textwidth,keepaspectratio]{images/discrim0.jpg}


## Дискриминантные оси

\blockbegin{Дискриминантные оси}

- задаются дискриминантными функциями
- вдоль них минимальное перекрывание групп
- дискриминантных осей всего на одну меньше чем групп (или столько же, сколько признаков, если признаков меньше, чем групп)

\blockend

\includegraphics{images/discrim.jpg}

## Дискриминантные функции

\blockbegin{Дискриминантные функции}

- описывают положение дискриминантных осей

\blockend

$$LD _j = d _{1j} X _{1} + d _{2i} X _{2} + ... + d _p X _{p}$$

- LD --- линейная дискриминантная функция
- d --- коэффициенты линейной дискриминантной функции
- X --- переменные-признаки
- j = 1, ... min(k--1, p) --- число дискриминантных функций
- p --- число признаков
- k --- число классов

\pause

### Стандартизованные коэффициенты дискриминантной функции

- используются для сравнения переменных, измеренных в разных шкалах используются стандартизованные коэффициенты дискриминантной функции
- большое абсолютное значение  --- большая дискриминирующая способность

## Классификация объектов

\columnsbegin
\column{0.48\textwidth}

\blockbegin{Функции классификации}

- Описывают __правдоподобие__ того, что объект с заданными свойствами относится к данной группе при данных значениях признаков согласно построенной классификации.

\blockend


$$C _{j} = c _{j0} + c _{j1} X _{1} + ... + c _{jp} X _{p}$$

- С --- функция классификации
- с --- коэффициенты функций классификации
- X --- переменные-признаки
- j = 1, ..., k --- число групп
- p --- число признаков

\column{0.48\textwidth}

Для каждого (в том числе, нового) объекта можно вычислить значение всех функций классификации. Какое значение больше --- к такой группе и надо отнести объект.

\includegraphics{images/discrim-bound.png}

Пример расположения областей принятия решений в линейном дискриминантном анализе с тремя группами

\columnsend

\tiny{Рис. с сайта http://statweb.stanford.edu/~jtaylo/courses/stats202/lda.html}


## Оценка качества классификации

\blockbegin{Таблица классификации}

- число верно или неверно классифицированных объектов (__confusion matrix__)

\blockend

Было / Стало | Класс А | Класс Б |
-------------|---------|---------|
Класс А      | верно   | неверно (Б вместо А)|
Класс Б      | неверно (A вместо Б) | верно |

## Проблема: как оценить качество классификации, чтобы можно было экстраполировать результаты?

Если оценить качество классификации на тех же данных, что были использованы для ее построения --- оценка неадекватная для классификации новых данных из-за "__переобучения__" (overfitting).

### Возможные решения проблемы переобучения

1. Разделить данные на __тренировочное и тестовое подмножества__:
  - тренировочные данные --- для подбора классификации (для обучения)
  - независимые тестовые данные --- для определения качества классификации

2. __Кроссвалидация__ --- разделение на тренировочное и тестовое подмножество повторяют многократно и усредняют оценки качества классификации между повторами.

## Требования к данным для дискриминантного анализа

- групп 2 или больше
- в каждой группе 2 и больше признаков
- число объектов должно быть больше, чем число признаков, лучше в несколько раз (в 4, например).
- признаки измерены в интервальной шкале

Кроме того, должны выполняться некоторые условия применимости (см. далее).

## Нужные пакеты и функции

```{r}
# Для дискриминантного анализа
library(MASS)
source("LDA_helper_functions.R")
# Графики
library(ggplot2)
# Чтение данных
library(readxl)
```



## Пример: Морфометрия ирисов

Сверхзадача --- научиться классифицировать ирисы по нескольким измерениям цветка

```{r}
data(iris)
head(iris, 10)
```

## По каким переменным легче всего различить группы?

Чтобы это узнать, построим графики всех пар переменных при помощи функции `pairs()` из базового пакета

```{r pairs-plot, eval=FALSE}
pairs(iris[, -5], col = iris$Species)
```

---

```{r pairs-plot, echo=FALSE, fig.height=7, out.height='3.5in', purl=FALSE}
```

Группы не различимы, если использовать любую из переменных отдельно, или даже какую-то пару переменных. Сделаем дискриминантный анализ.


## I. Дискриминантный анализ на тренировочных и тестовых данных

## 1) Разделяем на тренировочные и тестовые данные

```{r}
# доля от объема выборки, которая пойдет в тренировочный датасет
smp_size <- floor(0.80 * nrow(iris))
# устанавливаем зерно для воспроизводимости результатов
set.seed(982)
# индексы строк, которые пойдут в тренировочный датасет 
in_train <- sample(sample(1:nrow(iris), size = smp_size))
```

## 2) На тренировочных данных получаем стандартизованные коэффициенты дискриминантных функций

```{r}
lda_tr_scaled <- lda(scale(iris[in_train, -5]), iris$Species[in_train])
# коэффициенты дискриминантных функций
lda_tr_scaled$scaling
```

По ним можно оценить вклады разных признаков в изменчивость вдоль дискриминантных осей.


## 3) На тренировочных данных получаем функции классификации

```{r}
lda_tr <- lda.class(iris[in_train, -5], iris$Species[in_train])
# Коэф. функций классификации
lda_tr$class.funs
```

По ним можно классифицировать объекты.

## 4) Оцениваем качество классификации на тренировочных данных

```{r}
lda_tr_pred <- predict(lda_tr)
table(iris$Species[in_train], lda_tr_pred$class)
```

- Какова доля неправильно классифицированных случаев?


## 5) Оценка качества классификации на тестовых данных

Самое важное, если мы хотим использовать классификацию для прогноза

```{r}
lda_test_pred <- predict(lda_tr, iris[-in_train, -5])
table(iris$Species[-in_train], lda_test_pred$class)
```

- Какова доля неправильно классифицированных случаев?

## 6) График классификации тестовых данных

Можно отметить неправильно классифицированные случаи своим цветом

```{r}
class_df <- data.frame(lda_test_pred$x, 
                       new = lda_test_pred$class, 
                       real = iris$Species[-in_train])
class_df$Group <- factor(paste(class_df$real, class_df$new, sep = " as "))

ggplot(data = class_df, aes(x = LD1, y = LD2)) + 
  geom_point(aes(colour = Group))
```

## II. Дискриминантный анализ с кроссвалидацией

## Кроссвалидация

```{r}
lda_cv <- lda(iris[, -5], iris$Species, CV = TRUE)
names(lda_cv)
table(iris$Species, lda_cv$class)
```

`lda_cv$class` --- показывает, как классифицированы строки, если классификация обучена по остальным данным

## График классификации

```{r}
ggplot(data = iris, aes(x = Petal.Length,
                        y = Sepal.Width,
                        colour = Species,
                        shape = lda_cv$class)) +
  geom_point(size = 3) +
  scale_shape_discrete("Classified as")
```


## Условия применимости дискриминантного анализа


## Условия применимости дискриминантного анализа

- __признаки независимы друг от друга__ (чтобы не было избыточности, чтобы можно было инвертировать матрицы). Именно поэтому дискр. анализ часто применяется после анализа главных компонент.
- внутригрупповые ковариации приблизительно равны
- распределение признаков --- многомерное нормальное

\pause

Если условия применимости нарушены:

- В некоторых случаях, дискриминантный анализ дает хорошо работающие классификации.

- Возможно, другие методы, с менее жесткими требованиями, дадут классификации лучшего качества (например, квадратичный дискриминантный анализ --- quadratic discriminant analysis, дискриминантный анализ с использованием ядер --- kernel discriminant analysis)


## Проверка условий применимости

В данном случае, как и во многих других, они не выполняются, но мы уже убедились, что классификация работает...


## Mногомерная нормальность

```{r}
x <- as.matrix(iris[, -5])
d <- mahalanobis(x, colMeans(x), cov(x))
qqplot(qchisq(ppoints(nrow(x)), df = ncol(x)), d,
  main="QQ график для оценки многомерной нормальности",
  ylab="Расстояние Махаланобиса")
abline(a = 0, b = 1)
```

## Гомогенность ковариационных матриц

\small

```{r}

BoxMTest(as.matrix(iris[, -5]), iris$Species)
```


<!-- ## Квадратичный дискриминантный анализ -->


<!-- ## Квадратичный дискриминантный анализ -->

<!-- ```{r} -->
<!-- qda_tr <- qda(iris[in_train, -5], iris$Species[in_train]) -->
<!-- qda_tr_pred <- predict(qda_tr) -->
<!-- table(qda_tr_pred$class, iris$Species[in_train]) -->
<!-- qda_test_pred <- predict(qda_tr, iris[-in_train, -5]) -->
<!-- table(qda_test_pred$class, iris$Species[-in_train]) -->
<!-- ``` -->

<!-- ## Задание: Поссумы -->

<!-- - При помощи дискриминантного анализа классифицируйте популяции поссумов -->
<!-- - Хорошо ли работает классификация? -->
<!-- - Выполняются ли условия применимости? -->

<!-- \columnsbegin -->
<!-- \column{0.48\textwidth} -->

<!-- \includegraphics{images/possum.jpg} -->

<!-- \tiny{possum by Hasitha Tudugalle on Flickr -->
<!-- https://www.flickr.com/photos/hasitha\_tudugalle/6037880962} -->

<!-- \column{0.48\textwidth} -->

<!-- ```{r} -->
<!-- data(possum) # данные из пакета DAAG -->
<!-- possum <-  -->
<!--   possum[complete.cases(possum), ] -->
<!-- ``` -->

<!-- \columnsend -->

<!-- \tiny {Данные Lindenmayer et al. (1995)} -->

## Задание: Пингвины

\columnsbegin
\column{0.48\textwidth}

\includegraphics{images/palmerpenguins.jpg}

\tiny{bluegio at deviantart.com}

\column{0.48\textwidth}

Морфометрия пингвинов Адели, Генту и Чинстрап (данные `penguins`, Horst et al. 2020).

- При помощи дискриминантного анализа классифицируйте виды пингвинов по морфологическим признакам
- Хорошо ли работает классификация?
- Выполняются ли условия применимости?

\columnsend

\small
```{r echo=TRUE}
# library(palmerpenguins)
# data(penguins)
penguins <- read_xlsx(path = "data/penguins.xlsx", sheet = "penguin data")
head(penguins, 2)
colnames(penguins)
```

## Take-home messages

- Дискриминантный анализ --- метод классификации объектов по правилам, выработанным на выборке объектов с заранее известной принадлежностью

- Качество классификации можно оценить по числу неверно классифицированных объектов. Чтобы не было "переобучения" можно:
  - Подобрать классификацию на тренировочных данных и проверить на тестовых
  - Использовать кроссвалидацию --- классификацию объектов по правилам полученным по остальным данным (без учета этих объектов)

- Для дискриминантного анализа нужно отбирать признаки, независимые друг от друга или создавать синтетические признаки при помощи анализа главных компонент.

- Если внутригрупповые ковариации признаков различаются, лучше применять квадратичный дискриминантный анализ.


## Дополнительные ресурсы

- Quinn, Keough, 2002, pp. 435--441 


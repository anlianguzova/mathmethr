---
title: "Дискриминантный анализ"
subtitle: "Математические модели в зоологии"
author: 
  - Марина Варфоломеева
  - Анастасия Лянгузова
company: 'Каф. Зоологии беспозвоночных, СПбГУ'
output:
  xaringan::moon_reader:
    self-contained: true
    lib_dir: libs
    css: [default, tamu-fonts, ninjutsu, "assets/xaringan-themer.css", "assets/xaringan.css", "assets/scrollable.css"]
    df_print: default
    nature:
      highlightStyle: vs
      highlightLines: true
      countIncrementalSlides: false
      titleSlideClass: [middle, left, inverse]
      beforeInit: "assets/macros.js"
    includes:
      in_header: "assets/xaringan_in_header.html"
      after_body: "assets/xaringan_after_body.html"
---

```{r setup, include = FALSE, cache = FALSE, purl = TRUE}
source("assets/xaringan_setup.R")
library(xaringanExtra)
use_tile_view()
use_scribble()
use_search(show_icon = FALSE)
use_progress_bar(color = "#98BF64", location = "bottom", height = "10px")
use_freezeframe()
# use_webcam()
# use_panelset()
# use_extra_styles(hover_code_line = TRUE)

# http://tachyons.io/docs/
# https://roperzh.github.io/tachyons-cheatsheet/
use_tachyons()
source('support_mathmethr.R')
```

class: middle, center, inverse

## Дискриминантный анализ

---

### Вы сможете

- провести линейный дискриминантный анализ с использованием обучающей выборки и проверить качество классификации на тестовых данных или с использованием кроссвалидации
- проверить условия применимости дискриминантного анализа

---

class: middle, center, inverse

## Дискриминантный анализ

---

## Дискриминантный анализ

> ### Дискриминантный анализ
>
>- метод классификации объектов с учителем (__supervised learning__), т.е. применяется, когда принадлежность объектов к группе заранее известна.


Задачи дискриминантного анализа:

- выяснить, какие признаки лучше всего позволяют классифицировать объекты
- выяснить правило классификации существующих объектов
- классификация новых объектов неизвестной принадлежности по этому правилу

---

## Дискриминантный анализ

Нужно найти такую ось, вдоль которой группы различаются лучше всего, с минимальным перекрыванием.

Как она может проходить?

![](images/discrim0.jpg)

---

## Дискриминантные оси

> ### Дискриминантные оси
>
> - задаются дискриминантными функциями
> - вдоль них минимальное перекрывание групп
> - дискриминантных осей всего на одну меньше чем групп (или столько же, сколько признаков, если признаков меньше, чем групп)

![](images/discrim.jpg)

---

## Дискриминантные функции

> ### Дискриминантные функции
>
> - описывают положение дискриминантных осей


$$LD _j = d _{1j} X _{1} + d _{2i} X _{2} + ... + d _p X _{p}$$

- LD --- линейная дискриминантная функция
- d --- коэффициенты линейной дискриминантной функции
- X --- переменные-признаки
- j = 1, ... min(k--1, p) --- число дискриминантных функций
- p --- число признаков
- k --- число классов

--

### Стандартизованные коэффициенты дискриминантной функции

- используются для сравнения переменных, измеренных в разных шкалах используются стандартизованные коэффициенты дискриминантной функции
- большое абсолютное значение  --- большая дискриминирующая способность

---

## Классификация объектов

.pull-left[

> ### Функции классификации
>
>- Описывают __правдоподобие__ того, что объект с заданными свойствами относится к данной группе при данных значениях признаков согласно построенной классификации.




$$C _{j} = c _{j0} + c _{j1} X _{1} + ... + c _{jp} X _{p}$$

- С --- функция классификации
- с --- коэффициенты функций классификации
- X --- переменные-признаки
- j = 1, ..., k --- число групп
- p --- число признаков
]

.pull-right[
Для каждого (в том числе, нового) объекта можно вычислить значение всех функций классификации. Какое значение больше --- к такой группе и надо отнести объект.

![](images/discrim-bound.png)

Пример расположения областей принятия решений в линейном дискриминантном анализе с тремя группами

.tiny[
Рис. с сайта http://statweb.stanford.edu/~jtaylo/courses/stats202/lda.html
]
]

---

## Оценка качества классификации

> ### Таблица классификации
>
> число верно или неверно классифицированных объектов (__confusion matrix__)

Было / Стало | Класс А | Класс Б |
-------------|---------|---------|
Класс А      | верно   | неверно (Б вместо А)|
Класс Б      | неверно (A вместо Б) | верно |

---

## Проблема: как оценить качество классификации, чтобы можно было экстраполировать результаты?

Если оценить качество классификации на тех же данных, что были использованы для ее построения --- оценка неадекватная для классификации новых данных из-за "__переобучения__" (overfitting).

### Возможные решения проблемы переобучения

1. Разделить данные на __тренировочное и тестовое подмножества__:
  - тренировочные данные --- для подбора классификации (для обучения)
  - независимые тестовые данные --- для определения качества классификации

2. __Кроссвалидация__ --- разделение на тренировочное и тестовое подмножество повторяют многократно и усредняют оценки качества классификации между повторами.

---

## Требования к данным для дискриминантного анализа

- групп 2 или больше
- в каждой группе 2 и больше признаков
- число объектов должно быть больше, чем число признаков, лучше в несколько раз (в 4, например).
- признаки измерены в интервальной шкале

Кроме того, должны выполняться некоторые условия применимости (см. далее).

---

## Нужные пакеты и функции

```{r}
# Для дискриминантного анализа
library(MASS)
source("LDA_helper_functions.R")
# Графики
library(ggplot2)
# Чтение данных
library(readxl)
```

---

## Пример: Морфометрия ирисов

Сверхзадача --- научиться классифицировать ирисы по нескольким измерениям цветка

```{r}
data(iris)
colnames(iris)
colnames(iris) <- c("s_len", "s_wid", "p_len", "p_wid", "Sp")
head(iris)
```

---

## По каким переменным легче всего различить группы?

Чтобы это узнать, построим графики всех пар переменных при помощи функции `pairs()` из базового пакета

```{r pairs-plot}
pairs(iris[, -5], col = iris$Sp)
```

Группы не различимы, если использовать любую из переменных отдельно, или даже какую-то пару переменных. Сделаем дискриминантный анализ.

---

class: middle, center, inverse

## I. Дискриминантный анализ на тренировочных и тестовых данных

---

## 1) Разделяем на тренировочные и тестовые данные

```{r}
# устанавливаем зерно для воспроизводимости результатов
set.seed(764737)
# доля от объема выборки, которая пойдет в тренировочный датасет
smp_size <- floor(0.80 * nrow(iris))
# индексы строк, которые пойдут в тренировочный датасет
in_train <- sample(1:nrow(iris), size = smp_size)
```

---

## 2) На тренировочных данных получаем стандартизованные коэффициенты главных компонент 

```{r}
pca_tr_scaled <- rda(iris[in_train, -5], scale = TRUE)
summary(pca_tr_scaled)
```

---

## 3) Извлекаем из тренировочных данных site scores по всем главным компонентам 

```{r}
sites_tr_pca <- scores(pca_tr_scaled, display = 'sites', choices = c(1:4),
                       scaling = 'sites', correlation = TRUE)
head(sites_tr_pca, 2)
```

---

## 4) На тренировочных данных получаем стандартизованные коэффициенты дискриминантных функций

```{r}
lda_tr_pca <- lda(sites_tr_pca, iris$Sp[in_train])
# коэффициенты дискриминантных функций
lda_tr_scaled$scaling
```

По ним можно оценить вклады разных признаков в изменчивость вдоль дискриминантных осей.

---

## 5) На тренировочных данных получаем функции классификации

```{r}
lda_tr <- lda.class(sites_tr_pca, iris$Sp[in_train])
# Коэф. функций классификации
lda_tr$class.funs
```

По ним можно классифицировать объекты.

---

## 6) Оцениваем качество классификации на тренировочных данных

```{r}
lda_tr_pred <- predict(lda_tr)
table(iris$Sp[in_train], lda_tr_pred$class)
```

- Какова доля неправильно классифицированных случаев?

---

## 7) График классификации тренировочных данных 

```{r}
class_df <- data.frame(lda_tr_pred$x,
                       gr = lda_tr_pred$class,
                       real_gr = iris$Sp[in_train])
ggplot(data = class_df, aes(x = LD1, y = LD2, colour = gr)) +
  geom_text(size = 3, aes(label = real_gr)) +
  theme(legend.position = "none")
```

---

## 8) Оценка качества классификации на тестовых данных

Самое важное, если мы хотим использовать классификацию для прогноза

```{r}
# получаем PCA на тестовых данных
pca_pred_scaled <- rda(iris[-in_train, -5], scale = TRUE)

# извлекаем site scores
sites_pred_pca <- as.data.frame(scores(pca_pred_scaled, display = 'sites', choices = c(1:4),
                                       scaling = 'sites', correlation = TRUE))

# делаем предсказания
lda_test_pred <- predict(lda_tr, sites_pred_pca)
table(iris$Sp[-in_train], lda_test_pred$class)
```

- Какова доля неправильно классифицированных случаев?

---

## 9) График классификации тестовых данных

Можно отметить неправильно классифицированные случаи своим цветом

```{r}
class_df <- data.frame(lda_test_pred$x,
                       new = lda_test_pred$class,
                       real = iris$Sp[-in_train])
class_df$Group <- factor(paste(class_df$real, class_df$new, sep = " as "))

ggplot(data = class_df, aes(x = LD1, y = LD2)) +
  geom_point(aes(colour = Group))
```

---

class: middle, center, inverse

## II. Дискриминантный анализ с кроссвалидацией

---

## Кроссвалидация

```{r}
# главные компоненты
pca_cv_scaled <- rda(iris[, -5], scale = TRUE)

# site scores
sites_cv_pca <- scores(pca_cv_scaled, display = 'sites', choices = c(1:4),
                       scaling = 'sites', correlation = TRUE)

# дискриминантый анализ и кросс-валидация
lda_cv <- lda(sites_cv_pca, iris$Sp, CV = TRUE)
names(lda_cv)
table(iris$Sp, lda_cv$class)
```

`lda_cv$class` --- показывает, как классифицированы строки, если классификация обучена по остальным данным

---

## График классификации

```{r}
ggplot(data = iris, aes(x = p_len,
                        y = s_wid,
                        colour = Sp,
                        shape = lda_cv$class)) +
  geom_point(size = 3) +
  scale_shape_discrete("Classified as")
```

---

class: middle, center, inverse

## Дискриминантый анализ главных компонент: пакет adegenet

---

## Определение количества осей главных компонент для конкретных данных

### Кросс-валидация 

```{r}
library(adegenet) # подгружаем библиотеку
iris_xval <- xvalDapc(iris[, -5], iris$Sp, n.pca.max = 300, training.set = 0.8,
                      result = "overall", center = TRUE, scale = TRUE,
                      n.pca = NULL, n.rep = 100, xval.plot = TRUE)
```

---

## Извлекаем подобранное количество осей главных компонент

```{r}
iris_xval$`Number of PCs Achieving Highest Mean Success`
iris_xval$`Number of PCs Achieving Lowest MSE`
iris_xval$`Root Mean Squared Error by Number of PCs of PCA` # здесь чем меньше значение, тем лучше 
```

---

## DPCA непосредственно

```{r}
iris_dapc <- dapc(iris[, -5], n.pca = 3, n.da = 2, iris$Sp) # здесь число осей, соответственно результатам кросс-валидации
iris_dapc
```

---

## Визуализация полученных результатов

### Классификация

```{r}
my_col <- c("#50A8FF", "#50FF7F", "#FF50D7") #выбираем красивые цвета
scatter(iris_dapc, col = my_col, xax = 1, yax = 2, cex = 2, scree.da=FALSE, legend = FALSE, grp = iris$Sp)
```

---

## Визуализация полученных результатов

### Графики предсказаний

.pull-left[
```{r}
assignplot(iris_dapc)
```
]

.pull-right[
```{r}
compoplot(iris_dapc, lab = "", 
          ncol = 1, col = my_col)
```
]

---

class: middle, center, inverse

## Условия применимости дискриминантного анализа

---

## Условия применимости дискриминантного анализа

- __признаки независимы друг от друга__ (чтобы не было избыточности, чтобы можно было инвертировать матрицы). Именно поэтому дискр. анализ часто применяется после анализа главных компонент.
- внутригрупповые ковариации приблизительно равны
- распределение признаков --- многомерное нормальное

--

Если условия применимости нарушены:

- В некоторых случаях, дискриминантный анализ дает хорошо работающие классификации.

- Возможно, другие методы, с менее жесткими требованиями, дадут классификации лучшего качества (например, квадратичный дискриминантный анализ --- quadratic discriminant analysis, дискриминантный анализ с использованием ядер --- kernel discriminant analysis)
]

---

## Проверка условий применимости

В данном случае, как и во многих других, они не выполняются, но мы уже убедились, что классификация работает...

---

## Mногомерная нормальность

```{r}
x <- as.matrix(iris[, -5])
d <- mahalanobis(x, colMeans(x), cov(x))
qqplot(qchisq(ppoints(nrow(x)), df = ncol(x)), d,
  main="QQ график для оценки многомерной нормальности",
  ylab="Расстояние Махаланобиса")
abline(a = 0, b = 1)
```

---

## Гомогенность ковариационных матриц

.small[

```{r, highlight.output=c(6)}
BoxMTest(as.matrix(iris[, -5]), iris$Sp)
```
]

--

Нет гомогенности :(

---

class: middle, center, inverse

## Квадратичный дискриминантный анализ 

---

## Квадратичный дискриминантый анализ

Идея --- та же, что в основе линейного дискриминантного анализа. Отличие в том, что мы не предполагаем, что средние матриц ковариаций должны быть идентичными у разных групп сравнения. При этом изменяется т.н. `decision boundary` --- граница, по которой определяется принадлежность объекта к той или иной группе. 

```{r lda_va_qda_code, echo=FALSE, purl=FALSE, fig.width=13, fig.height=6}
library(caret)
library(MASS)
library(tidyverse)

## Code from https://www.geeksforgeeks.org/quadratic-discriminant-analysis/
decision_boundary = function(model, data,vars, resolution = 200,...) {
  class='Sp'
  labels_var = data[,class]
  k = length(unique(labels_var))
  # For sepals
  if (vars == 'sepal'){
  data = data %>% select(s_len, s_wid)
  }
  else{
  data = data %>% select(p_len, p_wid)
  }
   
   
  # plot with color labels
  int_labels = as.integer(labels_var)
  plot(data, col = int_labels+1L, pch = int_labels+1L, ...)
   
  # make grid
  r = sapply(data, range, na.rm = TRUE)
  xs = seq(r[1,1], r[2,1], length.out = resolution)
  ys = seq(r[1,2], r[2,2], length.out = resolution)
  dfs = cbind(rep(xs, each=resolution), rep(ys, time = resolution))
   
  colnames(dfs) = colnames(r)
  dfs = as.data.frame(dfs)
   
  p = predict(model, dfs, type ='class')
  p = as.factor(p$class)
 
   
  points(dfs, col = as.integer(p)+1L, pch = ".")
   
  mats = matrix(as.integer(p), nrow = resolution, byrow = TRUE)
  contour(xs, ys, mats, add = TRUE, lwd = 2, levels = (1:(k-1))+.5)
   
  invisible(mats)
}

par(mfrow=c(2,2))
# run the linear discriminant analysis and plot the decision boundary with Sepals variable
model = lda(Sp ~ s_len + s_wid, data=iris)
lda_sepals = decision_boundary(model, iris, vars= 'sepal' , main = "LDA_Sepals")
 
# run the quadratic discriminant analysis and plot the decision boundary with Sepals variable
model_qda = qda(Sp ~ s_len + s_wid, data=iris)
qda_sepals = decision_boundary(model_qda, iris, vars= 'sepal', main = "QDA_Sepals")
 
# run the linear discriminant analysis and plot the decision boundary with Petals variable
model = lda(Sp ~ p_len + p_wid, data=iris)
lda_petal =decision_boundary(model, iris, vars='petal', main = "LDA_Petals")
 
# run the quadratic discriminant analysis and plot the decision boundary with Petals variable
model_qda = qda(Sp ~ p_len + p_wid, data=iris)
qda_petal = decision_boundary(model_qda, iris, vars='petal', main = "QDA_Petals")
```


---

## Квадратичный дискриминантный анализ в R

```{r}
# Тренировочные данные
qda_tr <- qda(sites_tr_pca, iris$Sp[in_train])
qda_tr_pred <- predict(qda_tr)
table(qda_tr_pred$class, iris$Sp[in_train])

# Тестовые данные
qda_test_pred <- predict(qda_tr, sites_pred_pca)
table(qda_test_pred$class, iris$Sp[-in_train])
```

---

## Кроссвалидация QDA

```{r}
qda_cv <- qda(sites_cv_pca, iris$Sp, CV = TRUE)
table(iris$Sp, lda_cv$class)
```

---

## График предсказаний QDA

```{r}
ggplot(data = iris, aes(x = p_len,
                        y = s_wid,
                        colour = Sp,
                        shape = lda_cv$class)) +
  geom_point(size = 3) +
  scale_shape_discrete("Classified as")
```



---

## Take-home messages

- Дискриминантный анализ --- метод классификации объектов по правилам, выработанным на выборке объектов с заранее известной принадлежностью

- Качество классификации можно оценить по числу неверно классифицированных объектов. Чтобы не было "переобучения" можно:
  - Подобрать классификацию на тренировочных данных и проверить на тестовых
  - Использовать кроссвалидацию --- классификацию объектов по правилам полученным по остальным данным (без учета этих объектов)

- Для дискриминантного анализа нужно отбирать признаки, независимые друг от друга или создавать синтетические признаки при помощи анализа главных компонент.

- Если внутригрупповые ковариации признаков различаются, лучше применять квадратичный дискриминантный анализ.

---

## Дополнительные ресурсы

- Quinn, Keough, 2002, pp. 435--441 


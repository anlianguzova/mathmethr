---
title: "Ординация и классификация с использованием мер сходства-различия"
subtitle: "Математические модели в зоологии"
author: 
  - Марина Варфоломеева
  - Анастасия Лянгузова
company: 'Каф. Зоологии беспозвоночных, СПбГУ'
output:
  xaringan::moon_reader:
    self-contained: true
    lib_dir: libs
    css: [default, tamu-fonts, ninjutsu, "assets/xaringan-themer.css", "assets/xaringan.css", "assets/scrollable.css"]
    df_print: default
    nature:
      highlightStyle: vs
      highlightLines: true
      countIncrementalSlides: false
      titleSlideClass: [middle, left, inverse]
      beforeInit: "assets/macros.js"
    includes:
      in_header: "assets/xaringan_in_header.html"
      after_body: "assets/xaringan_after_body.html"
---

```{r setup, include = FALSE, cache = FALSE, purl = TRUE}
source("assets/xaringan_setup.R")
library(xaringanExtra)
use_tile_view()
use_scribble()
use_search(show_icon = FALSE)
use_progress_bar(color = "#98BF64", location = "bottom", height = "10px")
use_freezeframe()
# use_webcam()
# use_panelset()
# use_extra_styles(hover_code_line = TRUE)

# http://tachyons.io/docs/
# https://roperzh.github.io/tachyons-cheatsheet/
use_tachyons()
source('support_mathmethr.R')
```

class: middle, center, inverse

## Меры сходства и различия, ординация, классификация

---

### Вы сможете

- Выбирать подходящий для данных коэффициент сходства/различия
- Представлять многомерные данные в меньшем числе измерений при помощи неметрического многомерного шкалирования
- Строить дендрограммы при помощи подходящего метода аггрегации

---

class: middle, center, inverse

## Коэффициенты сходства и различия

---

## Коэффициенты сходства и различия

.pull-left[
**Различия (dissimilarities)**

$d \ge 0$

```{r echo=FALSE, fig.width=4.5, fig.height=0.5}
library(DiagrammeR)
DiagrammeR("images/dist.gv", type = "grViz")
# system('dot -Tpdf images/dist.gv -o images/dist.pdf')
```

]

.pull-right[
**Сходства (similarities)**

$0 \le S \le 1$ или $-1 \le S \le 1$

```{r echo=FALSE, fig.width=4.5, fig.height=0.5}
DiagrammeR("images/sim.gv", type = "grViz")
```

]

<br>

- Используются в качестве исходных данных для многих видов многомерных анализов, в т.ч. для неметрического многомерного шкалирования и некоторых видов кластерного анализа
- Из сходств можно получить расстояния и наоборот
- Свои коэффициенты для количественных и качественных признаков

---

## Свойства коэффициентов сходства-различия

.pull-left-33[
**Метрики и полуметрики**

Адекватность: $d_{A, A} = 0$

<!-- ```{r, echo=FALSE, fig.width=4, fig.height=.5} -->
<!-- DiagrammeR("images/adequa.gv", type = "grViz") -->
<!-- # system('dot -Tpdf images/adequa.gv -o images/adequa.pdf') -->
<!-- ``` -->

![](images/adequa.pdf)


Симметричность: $d_{A, B} = d_{B, A}$

```{r, echo=FALSE, fig.width=4, fig.height=.5}
# DiagrammeR("images/symmetry.mmd")
DiagrammeR("images/symmetry.gv", type = "grViz")
# system('dot -Tpdf images/symmetry.gv -o images/symmetry.pdf')
```

![](images/symmetry.pdf)
]

.pull-right-66[

.pull-left[
**Только метрики**

Триангулярность: $d_{A, B} \le d_{A, C} + d_{C, B}$

```{r, echo=FALSE, fig.width=4, fig.height=1}
DiagrammeR("images/triang.gv", type = "grViz")
# system('dot -Tpdf images/triang.gv -o images/triang.pdf')
```

![](images/triang.pdf)
]

.pull-right[
**Неметрики**

Все остальное
]
]

---

## Свойства коэффициентов сходства-различия

.pull-left[

**Нестандартные**

$$-\infty \le d \le \infty$$
]

.pull-right[

**Стандартные**

$$d_{min} \le d \le d_{max}$$

- частный случай стандартных коэффициентов - коррелятивные коэффициенты сходства

$$-1 \le S \le 1$$
]

---

## Примеры коэффициентов сходства-различия

.pull-left[
**Метрики (расстояния, distances):**

- без стандартизации:
    - Евклидово расстояние
    - Манхеттен (расстояние городских кварталов)

- со стандартизацией:
    - Канберра
    - хи-квадрат
    - Евклидово расстояние, рассчитанное по стандартизованным данным

**Полуметрики:**

- расстояние Махаланобиса
]

.pull-right[
**Неметрики:**

- со стандартизацией:
    - коррелятивные:
        - корреляция Пирсона
    - некоррелятивные:
        - коэффициент Брея-Куртиса
]

---

## Если количественные признаки измерены в одинаковых шкалах

**Метрики без стандартизации**

.pull-left[
- Евклидово расстояние

![](images/dist-euclid.png)
]

.pull-right[
Неевклидовы метрики

- Квадрат Евклидова расстояния

![](images/dist-euclid-sq.png)

- Манхэттеновское расстояние

![](images/dist-manhat.png)

]

---

## Если количественные признаки измерены в разных шкалах

**Можно стандартизовать исходные данные**

  - Евклидово (или другое) расстояние, рассчитанное по стандартизованным данным

**Можно использовать коэффициенты со стандартизацией**

- Канберра (метрика) $d = {\sum \frac {|x _{ik} - x _{jk}|} {|x _{ik}|+|x _{jk}|}}$
- хи-квадрат (метрика) $\chi^2 = \sqrt {\sum{ {\frac {1} {c _k}} (x _{ik} - x _{jk})^2 }}$
- Коэффициент Махаланобиса (неметрика) $d = \frac {\sum (x _{ik} - x _{jk})} {\sigma^2}$

- Корреляция Браве-Пирсона (коррелятивный) $S = \frac {\sum {(x _{ik} - \bar x _{i})(x _{ik} - \bar x _{j})}} {n \sigma^2 _{i} \sigma^2 _{j}}$

- Коэффициент Брея-Куртиса (не метрика) $BC _{ij} = \frac { 2C _{ij}} {S _i + S _j}$,
где $C _{ij}$ - сумма минимальных значений из тех, которые не равны нулю для обоих объектов, $S _i$ и $S _j$ - общее число ненулевых значений признаков для обоих объектов.

---

## Если признаки --- подсчеты численности

**Можно стандартизовать исходные данные**

Простая стандартизация не подходит (у счетных признаков не может быть среднее 0)

Можно использовать трансформации:

- корень, корень 4-й степени
- логарифмирование со сдвигом (log10(x + 1))

**Можно использовать коэффициенты со стандартизацией**

- Канберра (метрика) $d = {\sum \frac {|x _{ik} - x _{jk}|} {|x _{ik}|+|x _{jk}|}}$

- хи-квадрат (метрика) $\chi^2 = \sqrt {\sum{ {\frac {1} {c _k}} (x _{ik} - x _{jk})^2 }}$

---

## Если признаки --- доли или проценты

- хи-квадрат (метрика) $\chi^2 = \sqrt {\sum{ {\frac {1} {c _k}} (x _{ik} - x _{jk})^2 }}$
- коэффициент Брея-Куртиса (не метрика) $BC _{ij} = \frac { 2C _{ij}} {S _i + S _j}$

- Евклидово расстояние $d = \sqrt {\sum{(x _{ik} + x _{jk})^2}}$

---

## Если используются бинарные данные (присутствие-отсутствие признака)

![](images/qual.png)

---

## Примеры коэффициентов для качественных данных

![](images/qual_head.png)

.small[

|Коэффициент | Формула | Источник |
|------|--------|--------|
| Simple matching | $\frac{a + d} {a + b + c + d}$ | Sokal and Michener, 1958 |
| Rogers and Tanimoto | $\frac{a + d} {a + 2b + 2c + d}$ | Rogers and Tanimoto, 1960 |
| Anderberg | $\frac{a + d} {a + 2(b + c)}$ | Anderberg, 1973 |
| Russel and Rao | $\frac{a} {a + b + c + d}$ | Russel and Rao, 1940 |
| Jaccard | $\frac{a} {a + b + c}$ | Jaccard, 1901 |
| Sorensen-__Dice__ | $\frac{2a} {2a + b + c}$ | Dice, 1945, Sorensen, 1948 |
]

И т.д.

---

## Если данные смешанные (качественные и количественные)

### Коэффициенты для смешанных данных

- расстояние Говера

---

class: center, middle, inverse

## Неметрическое многомерное шкалирование

---

## Неметрическое многомерное шкалирование визуализирует отношения между объектами на основе расстояний между ними

.pull-left[

```{r, gg-eu, message = FALSE, echo = FALSE, fig.height = 6, fig.width = 5, purl=FALSE}
# data(eurodist)
# cities <- attr(eurodist, "Labels")
# library(geonames)
# options(geonamesUsername="varmara")
# GNsearchMy <- function(x){
#   res <- GNsearch(name = x, continentCode = "EU", cities = "cities1000")
#   return(res[1, c("name", "lng", "lat", "population")])
#   }
# GNresult <- sapply(cities, GNsearchMy, simplify = T)
# eu_coord <- data.frame(t(GNresult))
# eu_coord[, 1] <- as.character(eu_coord[, 1])
# eu_coord[, 2:4] <- sapply(eu_coord[, 2:4], function(x){
#   x <- as.numeric(x)
# return(x)
# })
# write.table(eu_coord, file = "data/cities.csv", quote = FALSE, sep = "\t")
eu_coord <- read.delim("data/cities.csv", stringsAsFactors = FALSE)
library(ggmap); library(mapproj)
theme_set(theme_bw(base_size = 14) + theme(legend.key = element_blank()))
update_geom_defaults("point", list(shape = 19, size = 3))
# map_dat <- get_map(location = 'Europe', zoom = 4, maptype = "satellite")
# save(map_dat, file = "data/09_dist_map_dat.RData")
load(file = "data/09_dist_map_dat.RData")
gg_eu <- ggmap(map_dat) + geom_point(aes(x = lng, y = lat, size = population/1000000), data = eu_coord, alpha = 0.8, colour = "red") + geom_text(aes(x = lng, y = lat, label = name), data = eu_coord, vjust = -0.3, hjust = -0.05, colour = "grey80", alpha = 0.8) + theme(legend.position = "bottom") + xlim(-10, 26) + ylim(36, 62) + labs(x = "Долгота", y = "Широта", size = "Население,\nмлн. чел.")
gg_eu
```
]

.pull-right[

Если бы мы знали расстояния по автодорогам между городами Европы

```{r, echo = FALSE, purl=FALSE}
data(eurodist)
as.dist(as.matrix(eurodist)[1:5, 1:5])
```

мы бы смогли восстановить по ним карту

```{r eu, echo = FALSE, fig.width=2, fig.height=2, out.width='2in', out.height='1.9in', purl=FALSE}
library(vegan)
euro <- metaMDS(eurodist, trace = 0)
op <- par(mar = c(3, 3, 0.1, 0.1), mgp = c(2, 1, 0), cex = 0.8)
invisible(tryCatch(ordiplot(euro, type = "t", cex = 1.2), error = function(er){}))
par(op)
```
]

---

## Неметрическое многомерное шкалирование

Неметрическое многомерное шкалирование (nonmetric multidimensional scaling, nMDS) --- метод визуализации отношений между объектами в пространстве с небольшим числом измерений.

Исходные данные --- матрица расстояний между объектами в многомерном пространстве.



.pull-left[

- nMDS подбирает расстояния между объектами на графике так, чтобы сохранились соотношение исходных расстояний между ними. Т.е. если исходно A и B были ближе, чем B и С, то и в результате они должны быть ближе, чем B и С.

- Ординацию nMDS можно поворачивать,  отражать, сдвигать - результат от этого не изменится.
]


.pull-right[

```{r eu, echo = FALSE, fig.width=4.5, fig.height=3.5, purl=FALSE}
```
]

---

## Пример: Морфометрия поссумов

.pull-left[
![](images/possum.jpg)

.tiny[
Possum by Hasitha Tudugalle on [Flickr](https://www.flickr.com/photos/hasitha\_tudugalle/6037880962)
]
]


.pull-right[

В датафрейме содержатся данные об измерениях поссумов в разных частях тела. Кроме того, есть ещё и факторные переменные: 

- site --- места, в которых поссумы были пойманы (7 локаций)

- Pop --- популяция, к которой поссумы относятся (`Victoria` или `other`)

- sex --- пол поссума (`f` или `m`)

.tiny[
Данные Lindenmayer et al. (1995)
]
]

---

## Знакомимся с данными

```{r}
library(DAAG)
data(possum)
colnames(possum)

colSums(is.na(possum))
# оставим только строки с полными наблюдениями
pos <- possum[complete.cases(possum), ]
# поссумы из разных сайтов из 2 популяций
table(pos$Pop, pos$site)
```

---

## Неметрическое многомерное шкалирование

Построим ординацию поссумов на основе их сходства по морфометрии и возрасту.

Функция metaMDS много раз итеративно подбирает координаты поссумов в новом пространстве (двумерном по умолчанию) и сохраняет лучшую конфигурацию.

`autotransform` --- если `TRUE`, то данные предварительно подвергаются двойной ("висконсинской") стандартизации (см. `?metaMDS`). __Если у вас не данные о сообществах, то это нужно отключить__

```{r}
library(vegan)
ord_euclid <- metaMDS(pos[, 6:14], distance = "euclid", autotransform = FALSE)
```

---

## Качество подгонки модели

__stress__ - оценивает, насколько были искажены исходные расстояния между объектами при снижении размерности

```{r}
ord_euclid$stress
```

- Эмпирическое правило: хорошо < 0.25 (или, иногда, 0.20) < плохо

---

## Ординация

.pull-left[
Координаты наблюдений:

```{r}
head(ord_euclid$points, 10)
```
]


.pull-right[
График ординации:

```{r fig-ugly-ord, fig.width = 4.5, fig.height = 5}
ordiplot(ord_euclid, type = "t")
```
]

---

## Задание 1

При помощи `ggplot2` постройте график неметрического многомерного шкалирования.  
Для графика используйте координаты точек `ord_euclid$points` и исходные данные.  
Раскрасьте график по значениям переменной `Pop`.  
Сделайте так, чтобы особи разного пола были изображены на разных панелях

```{r fig-ord, echo=FALSE, eval=TRUE, purl=FALSE}
library(ggplot2)
# Данные для графика
points_euclid <- data.frame(pos, ord_euclid$points)
# График nMDS ординации
gg_euclid <- ggplot(points_euclid, aes(x = MDS1, y = MDS2)) +
  geom_point(aes(colour = Pop), alpha = 0.5) + 
  facet_wrap(~sex)
gg_euclid
```

---

## Дополните код

```{r eval=FALSE, purl=TRUE}
library()
# Данные для графика
points_euclid <- data.frame( , )
# График nMDS ординации
gg_euclid <- ggplot(, aes(x = , y = )) +
  geom_point() + 
  facet_wrap(~ )
gg_euclid
```

---

## Решение: график ординации

```{r fig-ord, echo=TRUE, purl=FALSE}
```

---

## Задание 2

Постройте nMDS ординацию при помощи евклидова расстояния, по стандартизованным данным

Дополните код

```{r eval=FALSE}
# Ординация
ord_scaled <- metaMDS( (pos), distance = , autotransform = )
# Качество ординации
```

---

## Решение:

.small[

```{r warning=FALSE}
# Ординация
ord_scaled <- metaMDS(scale(pos[, 6:14]), distance = "euclide", autotransform = FALSE)
# Качество ординации
ord_scaled$stress
```
] 

Для сравнения, стресс был
```{r purl=FALSE}
ord_euclid$stress
```

Похоже, что в этом случае лучшая ординация была получена при использовании евклидова расстояния без стандартизации --- у нее меньше значение стресса.

Но сейчас мы продолжим: сравним графики обеих ординаций.

---

## График ординации по матрице евклидовых расстояний, рассчитанных по стандартизованным данным

```{r fig-ord-raw, fig.width=10}
# Данные для графика
points_scaled <- data.frame(ord_scaled$points, pos)
# График nMDS-ординации
gg_scaled <- gg_euclid %+% points_scaled
gg_scaled
```

---

## Видно, что графики ординации, полученные разными методами, различаются

```{r gg-both, echo=FALSE, purl=TRUE, fig.height=5, purl=FALSE}
library(gridExtra)
grid.arrange(gg_euclid + aes(size = age),
             gg_scaled + aes(size = age),
             ncol = 1)
```

---

## Код для графиков

```{r gg-both, echo=TRUE, eval=FALSE, purl=TRUE}
```

---

class: middle, center, inverse

## Кластерный анализ

---

## Пример: Морфометрия самок поссумов

```{r}
# library(DAAG)
data(fossum)
```

Данные для кластерного анализа нужно подготовить:

- создать осмысленные имена строк
- выбрать только переменные, нужные для построения матрицы сходств-различий
- выбрать только строки без пропусков

---

## Создаем "говорящие" названия строк

Сейчас в названиях строк записано из какой точки каждый поссум
```{r}
rownames(fossum) # было
```

Чтобы имена строк были более информативны, добавим к ним название популяции

```{r}
rownames(fossum) <- paste(fossum$Pop, 
                          rownames(fossum), 
                          sep = "_")
rownames(fossum) # стало
```

---

## Отбираем только то, что понадобится для кластеризации

Отбираем  
только строки без пропущенных значений, 
и только столбцы с морфометрическими данными

```{r}
fos <- fossum[complete.cases(fossum), 6:14]
```

---

## Методы кластеризации

.pull-left[
### Иерархические методы

- методы построения деревьев (о них следующие слайды)
]

.pull-right[
### Неиерархические методы

- кластеризация K-means 
- кластеризация C-means 
- основанная на плотности пространственная кластеризация для приложений с шумами (DBSCAN)
- упорядочение точек для обнаружения кластерной структуры (OPTICS)

]

---

## Какие бывают методы построения деревьев?

.pull-left[
### Методы класстеризации на основании расстояний (о них сегодня пойдет речь)
  - Метод ближайшего соседа (single linkage)
  - Метод отдаленного соседа (complete linkage)
  - Метод среднегруппового расстояния (average linkage, UPGMA)
  - Метод Варда (Ward's method)
  - Метод присоединения соседей (Neighbour Joining)

Эти методы есть в базовом пакете `stats`, и в пакете `ape`. Разные полезные функции есть в `ade4` и `adegenet`
]

.pull-right[
### Методы кластеризации на основании признаков
  - Метод максимальной бережливости 
  - Метод максимального правдоподобия
  
Эти методы реализованы в пакете `phangorn` 

Со списком пакетов для филогенетического анализа в R можно познакомиться здесь:  
https://cran.r-project.org/web/views/Phylogenetics.html
]

---

class: middle, center, inverse

## Методы класстеризации на основании расстояний

---

## От чего зависит результат кластеризации

Результат кластеризации зависит от

- коэффициента сходства-различия
- от алгоритма кластеризации

---

## Кластерный анализ начинается с расчета матрицы расстояний между объектами

Далее мы будем использовать матрицу евклидовых расстояний между поссумами.

```{r}
d <- dist(x = fos, method = "euclidean")
```

Давайте построим деревья при помощи нескольких алгоритмов кластеризации и сравним их.

---

## Методы кластеризации

```{r gg-all, echo=FALSE, fig.height=5, purl=FALSE}
cl_dat <- data.frame(cl = c(rep("A", 5), rep("B", 4)), 
           x = c(1, 2.7, 2, 1.5, 2, 5, 6, 5.5, 5.8),
           y = c(1, 1.2, 3, 2, 1.5, 1.2, 1, 3, 2))

segm_between <- function(ind1, ind2, dat){
i_betw <- expand.grid(ind1, ind2)
segm <- lapply(1:nrow(i_betw), function(i) cbind(dat[i_betw[i, 1], ], dat[i_betw[i, 2], ]))
segm <- Reduce(rbind, segm)
colnames(segm) <- c("x", "y", "xend", "yend")
return(segm)
}

segm_within <- function(ind1, ind2, dat){
  # for ward
  dat1 <- dat[ind1, ]
  dat2 <- dat[ind2, ]
with1 <- segm_between(1:nrow(dat1), nrow(dat1)+1, rbind(dat1, colMeans(dat1)))
with2 <- segm_between(1:nrow(dat2), nrow(dat2)+1, rbind(dat2, colMeans(dat2)))
segm <- rbind(with1, with2)
return(segm)
}

betw_segm <- segm_between(1:5, 6:9, cl_dat[, 2:3])
with_segm <- segm_within(1:5, 6:9, cl_dat[, 2:3])

library(dplyr)
cl_means <- cl_dat %>% group_by(cl) %>% summarise(
  x = mean(x), y = mean(y)
)
betw <- as.matrix(dist(cl_dat[, 2:3]))[6:9, 1:5]
# which.min(betw)
# which.max(betw)
th <- theme_classic() + theme(axis.line = element_blank(), axis.title = element_blank(), axis.ticks = element_blank(), axis.text = element_blank(), legend.position = "none")

gg <- ggplot(cl_dat, aes(x = x, y = y, colour = cl)) + geom_point() + stat_ellipse(level = 0.8) + geom_point(data = cl_means, size = 4, shape = 5) + th

gg_single <- gg +  annotate(geom = "segment", x = 2.7, y = 1.2, xend = 5, yend = 1.2, colour = "grey60")

gg_complete <- gg +  annotate(geom = "segment", x = 1, y = 1, xend = 6, yend = 1, colour = "grey60")

gg_average <- gg + geom_segment(data = betw_segm, aes(x = x, y = y, xend = xend, yend = yend, colour = NULL), colour = "grey60")

gg_ward <- gg + geom_segment(data = with_segm, aes(x = x, y = y, xend = xend, yend = yend, colour = NULL), colour = "grey60")

grid.arrange(gg_single + ggtitle("Метод ближайшего соседа"), gg_complete + ggtitle("Метод отдаленного соседа"), gg_average + ggtitle("Метод среднегруппового расстояния"), gg_ward + ggtitle("Метод Варда"), ncol = 2)
```

---

## Метод ближайшего соседа

.pull-left[
- = nearest neighbour = single linkage
- к кластеру присоединяется ближайший к нему кластер/объект
- кластеры объединяются в один на расстоянии, которое равно расстоянию между ближайшими объектами этих кластеров
]

.pull-right[
```{r gg-single, echo=FALSE, fig.width=4.5, purl=FALSE}
gg_single
```
]

**Особенности:**

- Может быть сложно интерпретировать, если нужны группы
  - объекты на дендрограмме часто не образуют четко разделенных групп
  - часто получаются цепочки кластеров (объекты присоединяются как бы по-одному)
- Хорош для выявления градиентов

---

## Метод ближайшего соседа в R

```{r fig-single, fig.height=4, fig.width=8}
hc_single <- hclust(d, method = "single")
library(ape)
ph_single <- as.phylo(hc_single)
# cex - относительный размер шрифта
plot(ph_single, type = "phylogram", direction = "downwards", cex = 0.7)
axisPhylo(side = 2)
```

---

## Метод отдаленного соседа

.pull-left[
- = furthest neighbour = complete linkage
- к кластеру присоединяется отдаленный кластер/объект
- кластеры объединяются в один на расстоянии, которое равно расстоянию между самыми отдаленными объектами этих кластеров (следствие - чем более крупная группа, тем сложнее к ней присоединиться)
]

.pull-right[
```{r gg-compl, echo=FALSE, fig.width=4.5, purl=FALSE}
gg_complete
```
]

**Особенности:**

- На дендрограмме образуется много отдельных некрупных групп
- Хорош для поиска дискретных групп в данных

---

## Метод отдаленного соседа в R

```{r fig-compl, fig.height=5, fig.width=10}
ph_compl <- as.phylo(hclust(d, method = "complete"))
plot(ph_compl, type = "phylogram", direction = "downwards", cex = 0.8)
axisPhylo(side = 2)
```

---

## Метод невзвешенного попарного среднего

.pull-left[
- = UPGMA = Unweighted Pair Group Method with Arithmetic mean
- кластеры объединяются в один на расстоянии, которое равно среднему значению всех возможных расстояний между объектами из разных кластеров.
]

.pull-right[
```{r gg-avg, echo=FALSE, fig.width=4.5, fig.height=2.5, purl=FALSE}
gg_average
```
]

**Особенности:**
UPGMA и WUPGMС иногда могут приводить к инверсиям на дендрограммах

![width=0.5\linewidth](images/clust-revert.png)

.tiny[
из Borcard et al., 2011
]

---

## Метод невзвешенного попарного среднего в R

```{r fig-avg, fig.height=5, fig.width=10}
ph_avg <- as.phylo(hclust(d, method = "average"))
plot(ph_avg, type = "phylogram", direction = "downwards", cex = 0.8)
axisPhylo(side = 2)
```

---

## Метод Варда

.pull-left[
- = Ward's Minimum Variance Clustering
- объекты объединяются в кластеры так, чтобы внутригрупповая дисперсия расстояний была минимальной
]


.pull-right[
```{r gg-ward, echo=FALSE, fig.width=4.5, fig.height=2.5, purl=FALSE}
gg_ward
```
]

**Особенности:**

- метод годится и для неевклидовых расстояний несмотря на то, что внутригрупповая дисперсия расстояний рассчитывается так, как будто это евклидовы расстояния

---

## Метод Варда в R

```{r fig-ward, fig.height=5, fig.width=10}
ph_w2 <- as.phylo(hclust(d, method = "ward.D2"))
plot(ph_w2, type = "phylogram", direction = "downwards", cex = 0.8)
axisPhylo(side = 2)
```

---

class: middle, center, inverse

## Cравнение и интерпретация результатов кластеризации

---

## Кофенетическая корреляция

Кофенетическое расстояние --- расстояние между объектами на дендрограмме.

Кофенетическую корреляцию можно рассчитать как пирсоновскую корреляцию (обычную) между матрицами исходных и кофенетических расстояний между всеми парами объектов. 

Метод, который дает наибольшую кофенетическую корреляцию, дает кластеры, лучше всего отражающие исходные данные.

---

## Кофенетическая корреляция в R

```{r}
# Кофенетические расстояния
c_single <- as.dist(cophenetic(ph_single))
c_compl <- as.dist(cophenetic(ph_compl))
c_avg <- as.dist(cophenetic(ph_avg))
c_w2 <- as.dist(cophenetic(ph_w2))
```

```{r}
# Кофенетические корреляции
cor(d, c_single)
cor(d, c_compl)
cor(d, c_avg) 
cor(d, c_w2)
```

---

## На каком уровне нужно делить дендрограмму на кластеры?

- Можно субъективно, на любом выбранном уровне. Главное, чтобы кластеры были осмысленными и интерпретируемыми.
- Можно выбрать, глядя на распределение расстояний ветвления
- Можно оценить вероятность разделения на кластеры при помощи бутстрепа

---

## Бутстреп-поддержка ветвей

Функция `system.time` - покажет, сколько времени заняли рассчеты

Аргументы `pvclust`:

- `nboot` --- число итераций должно быть больше 10000. В примере мы используем мало для скорости
- `parallel = TRUE` --- проводить параллельные вычисления на нескольких ядрах процессора. Ускоряет расчеты
- `iseed` --- зерно генератора случайных чисел для вычислений. Обязательно задавайте этот аргумент, если хотите, чтобы вычисления воспроизводились при повторных запусках

```{r}
library(pvclust)
```

```{r pvclust-compute, cache=TRUE}
system.time({
cl_boot <- pvclust(scale(t(fos)), 
                   method.hclust = "average", 
                   method.dist = "euclidean",
                   nboot = 5000,
                   parallel = TRUE,
                   iseed = 42)
})
```

---

## Бутстреп-поддержка ветвей 

```{r fig-pvclust, fig.width=10, fig.height=7}
plot(cl_boot, cex.pv = 0.8, cex = 0.8)
```

---

class: middle, center, inverse

## Построение деревьев по генетическим данным

---

## Teaser

В этом курсе нет возможности рассказать даже о малой доле возможностей R для работы с генетическими данными, поэтому давайте сделаем небольшую демонстрацию.

---

## Пример: Митохондриальная ДНК приматов.

В файле primates.dna содержатся последовательности участка митохондриальной ДНК. для 12 видов приматов. Последовательности для мыши и коровы --- в качестве аутгруппы. (232bp в контрольном участке плюс третий кодон в близлежащих белок-кодирующих митохондриальных генах --- 1-2 кодоны исключены в попытке получить сходную скорость эволюции во всех сайтах)

Датасет собан Dr. Masami Hasegawa (Institute of Statistical Mathematics, Tokyo), по данным сиквенирования Kenji Hayasaka, Takashi Gojobori, Satoshi Horai (Molecular Biology and Evolution 5: 626-644, 1988).

Исходный файл в формате PHYLIP можно загрузить по ссылке: http://evolution.genetics.washington.edu/book/primates.dna

---

## Дерево по генетическим данным

```{r eval=FALSE}
webpage <-"http://evolution.genetics.washington.edu/book/primates.dna"
primates.dna <- read.dna(webpage)
d_pri <- dist.dna(primates.dna, model = "K80")
hc_pri <- hclust(d_pri, method = "average")
ph_pri <- as.phylo(hc_pri)
plot(ph_pri, cex = 0.8)
axisPhylo()
```

```{r fig-gen-tree, fig.height = 4, fig.width=8, out.height='2in', out.width='4.5in', echo=FALSE, purl=FALSE}
# webpage <-"http://evolution.genetics.washington.edu/book/primates.dna"
# primates.dna <- read.dna(webpage)
# save(primates.dna, file = "data/09_dist_primates.dna.RData")
load(file = "data/09_dist_primates.dna.RData")
d_pri <- dist.dna(primates.dna, model = "K80")
hc_pri <- hclust(d_pri, method = "average")
ph_pri <- as.phylo(hc_pri)
plot(ph_pri, cex = 0.8)
axisPhylo()
```

---

class: middle, center, inverse

## Неиерархические методы кластеризации

---

## K-means кластеризация

.pull-left[
```{r purl=FALSE, echo=FALSE}
data_k_final <- data.frame(X = c(1.4, 1.8, 2.5, 3, 2.3, 1.8,
                           3, 2.8, 3.5, 4, 4.2,
                           7, 8.2, 7.5, 8.4, 8, 7.7, 9),
                     Y = c(1, 1.5, 3.7, 2.8, 2.5, 3,
                           4.8, 5.2, 7, 6.3, 5.9,
                           5, 5.2, 5.8, 6.7, 6.3, 7, 6.9),
                     Col = c(1, 1, 1, 1, 1, 1,
                             2, 2, 2, 2, 2,
                             3, 3, 3, 3, 3, 3, 3))

gg_k_final <-  ggplot(data_k_final, aes(X, Y, col = factor(Col))) +
  geom_point(size = 3) +
  theme_bw() +
  scale_colour_manual(values = c("#A1FFA1", "#A1C7FF", "#FFA1DA")) +
  annotate("text", x = mean(data_k_final$X[1:6]), y = mean(data_k_final$Y[1:6]), 
           label = "X") +
  annotate("text", x = mean(data_k_final$X[7:11]), y = mean(data_k_final$Y[7:11]), 
           label = "X") +
  annotate("text", x = mean(data_k_final$X[12:18]), y = mean(data_k_final$Y[12:18]), 
           label = "X") +
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        legend.position = "none")

gg_k_final
```
]

.pull-right[
В отличие от иерархических методов кластеризации K-means будет искать то количество кластеров, которое вы ему зададите (в количестве k штук). Каждое наблюдение принадлежит кластеру с ближайшим значением среднего числа (центроида); помимо этого K-means кластеризация минимизирует разброс значений внутри каждого из кластера.

Используется в машинном обучении, в том числе, например, для цветовой редуцкии изображений. 
]

---

## Алгоритм K-means кластеризации

.pull-left[
### График наблюдений 
```{r purl = FALSE, echo=FALSE}
data_k <- data.frame(X = c(1.4, 1.8, 2.5, 3, 2.3, 1.8,
                           3, 2.8, 3.5, 4, 4.2,
                           7, 8.2, 7.5, 8.4, 8, 7.7, 9),
                     Y = c(1, 1.5, 3.7, 2.8, 2.5, 3,
                           4.8, 5.2, 7, 6.3, 5.9,
                           5, 5.2, 5.8, 6.7, 6.3, 7, 6.9),
                     Col = c(1, 0, 0, 0, 2, 0,
                             0, 0, 0, 0, 0,
                             0, 0, 0, 0, 3, 0, 0),
                     Size = c(1, 0, 0, 0, 1, 0,
                              0, 0, 0, 0, 0,
                              0, 0, 0, 0, 1, 0, 0))

gg_kmeans <- ggplot(data_k, aes(X, Y)) +
  geom_point(size = 3) + theme_bw()
gg_kmeans
```

Здесь как будто бы выделяются 3 кластера, поэтому возьмём k = 3. Что же будет делать алгоритм?
]

--

.pull-right[
### 1. Выбираются случайным образом 3 точки на графике --- кластерные центроиды

Например, так:

```{r purl = FALSE, echo=FALSE}
gg_centr <- gg_kmeans +
  geom_point(aes(col = factor(Col), size = Size)) +
  scale_colour_manual(values = c("#000000", "#A1FFA1",
                                "#A1C7FF", "#FFA1DA")) +
  scale_size(range = c(3, 8)) +
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        legend.position = "none")

gg_centr
```
]

---

## Алгоритм K-means кластеризации 

.pull-left[
### 2. Измеряется Евклидово расстояние между каждой точкой и центроидом 

При этом каждая точка приписывается к ближайшему кластеру. 

```{r purl=FALSE, echo=FALSE}
data_k2 <- data.frame(X = c(1.4, 1.8, 2.5, 3, 2.3, 1.8,
                           3, 2.8, 3.5, 4, 4.2,
                           7, 8.2, 7.5, 8.4, 8, 7.7, 9),
                     Y = c(1, 1.5, 3.7, 2.8, 2.5, 3,
                           4.8, 5.2, 7, 6.3, 5.9,
                           5, 5.2, 5.8, 6.7, 6.3, 7, 6.9),
                     Col = c(1, 1, 0, 2, 2, 0,
                             0, 0, 0, 0, 0,
                             0, 0, 0, 3, 3, 0, 0),
                     Size = c(1, 0, 0, 0, 1, 0,
                              0, 0, 0, 0, 0,
                              0, 0, 0, 0, 1, 0, 0))


gg_nearest <- ggplot(data_k2, aes(X, Y, col = factor(Col), size = Size)) +
  geom_point() +
  theme_bw() +
  scale_colour_manual(values = c("#000000", "#A1FFA1",
                                "#A1C7FF", "#FFA1DA")) +
  scale_size(range = c(3, 8)) +
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        legend.position = "none")

gg_nearest
```
]

.pull-right[
### 3. Расситываются центроиды для каждого кластера

```{r purl=FALSE, echo=FALSE}
data_k3 <- data.frame(X = c(1.4, 1.8, 2.5, 3, 2.3, 1.8,
                            3, 2.8, 3.5, 4, 4.2,
                            7, 8.2, 7.5, 8.4, 8, 7.7, 9),
                      Y = c(1, 1.5, 3.7, 2.8, 2.5, 3,
                            4.8, 5.2, 7, 6.3, 5.9,
                            5, 5.2, 5.8, 6.7, 6.3, 7, 6.9),
                      Col = c(1, 1, 2, 2, 2, 2,
                              2, 2, 3, 3, 3,
                              3, 3, 3, 3, 3, 3, 3),
                      Size = c(1, 0, 0, 0, 1, 0,
                               0, 0, 0, 0, 0,
                               0, 0, 0, 0, 1, 0, 0))


gg_new_centr <- ggplot(data_k3, aes(X, Y, col = factor(Col), size = Size)) +
  geom_point() +
  theme_bw() +
  scale_colour_manual(values = c("#A1FFA1",
                                 "#A1C7FF", "#FFA1DA")) +
  scale_size(range = c(3, 8)) +
  annotate("text", x = mean(data_k3$X[1:2]), y = mean(data_k3$Y[1:2]), label = "X") +
  annotate("text", x = mean(data_k3$X[3:8]), y = mean(data_k3$Y[3:8]), label = "X") +
  annotate("text", x = mean(data_k3$X[9:18]), y = mean(data_k3$Y[9:18]), label = "X") +
  theme(axis.text.x = element_blank(),
          axis.ticks.x = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks.y = element_blank(),
    legend.position = "none")

gg_new_centr
```
]

---

## 4. Расчёт расстояний от каждой точки до нового центроида 

.pull-left[
Также оценивается разброс внутри каждого кластера.

```{r purl=FALSE, echo=FALSE}
gg_dist_new <-  gg_new_centr +
  geom_segment(aes(X[1], y = Y[1], xend = mean(X[1:2]), yend = mean(Y[1:2])),
               linewidth = 0.5, colour = "black") +
  geom_segment(aes(X[4], y = Y[4], xend = mean(X[3:8]), yend = mean(Y[3:8])),
               linewidth = 0.5, colour = "black") +
  geom_segment(aes(X[12], y = Y[12], xend = mean(X[9:18]), yend = mean(Y[9:18])),
               linewidth = 0.5, colour = "black")

gg_dist_new
```
]

.pull-right[
Разброс считается как сумма квадратов расстояний между отдельными наблюдениями и центроидом.

$$ \sum_{i=1}^{n}(x_i - \overline{x})$$
]

---

## 5. Повторяем всё многократно до тех пор, пока разброс не станет минимальным

Кластеры с минимальным разбросом --- финальные. 

```{r purl=FALSE, echo=FALSE}
gg_k_final
```

---

## Подбор количества кластеров

А что, если на глаз не видно точное количество кластеров?.. 

```{r purl=FALSE, echo=FALSE}
gg_kmeans
```

Может быть тут два кластера?.. Чтобы определить оптимальное количество нам также понадобится сумма квадратов была минимальной. 

Чтобы разобраться, перейдём к коду в R. 

---

# K-means в R

.pull-left[
В первую очередь стандартизуем данные.

```{r}
f_sc <- scale(fos)
```

Визуализацию изменения суммы квадратов можно посмотреть с помощью функции `fviz_nbclust` из пакета `factoextra`.
]

.pull-right[
```{r}
fviz_nbclust(f_sc, kmeans, method = "wss")
```
]

Определяем точку (в английской литературе её называют elbow), после которой менее резко начинает изменяться within sum of squares. В нашем случае это 4. 

---

## K-means кластеризация и визуализация

```{r}
f_kmeans <- kmeans(f_sc, centers = 4, nstart = 20) # проводим кластеризацию

my_col <- c("#2E9FDF", "#FF5AD9", "#B5FF0A", "#F37352") # создаём вектор цветов для раскраски

# визуализируем
fviz_cluster(f_kmeans, data = f_sc,
             palette = my_col,
             ggtheme = theme_bw())
```

---

## Density-based spatial clustering of applications with noise (DBSCAN)

.pull-left[
Основанная на плотности пространственная кластеризация для приложений с шумами --- метод, более подходящий для "вложенных" кластеров. Основан на распределении плотности точек. 

```{r purl=FALSE, echo=FALSE}
library(dbscan)
library(factoextra)
library(dplyr)

data('multishapes')

multi_circle <- multishapes %>% filter(shape == c(1, 2))
multi <- multi_circle[, 1:2]

ggplot(multi, aes(x, y)) + geom_point()
```
]

.pull-right[
Работает с данными, с которыми K-means справиться не может. Как, например, тут. 
```{r}
set.seed(123)
circle_kmeans <- kmeans(multi, centers = 2, nstart = 20)
my_col_circle <- c("#2E9FDF", "#E7B800")
fviz_cluster(circle_kmeans, data = multi,
             palette = my_col_circle)
```
]

---

## Принцип работы DBSCAN 

Кластеры выбираются на основе плотности расположения точек. В результате в единый кластер объединяются близко расположенные друг к другу точки. 
Исследователем задаются два важных параметра для объединения точек в один кластер:
- радиус расстояния, на котором должны рассматриваться близлежащие точки (`eps`)
- минимальное количество точек, которые расположены в круге этого радиуса (`minPts`)

Сore points --- точки, от которых можем присоединять в кластер новые точки и рядом с которыми расположено `minPts` количество точек. Есть ещё пограничные точки --- те, на которых кластер заканчивается. Остальные точки считаются шумом и выбросами.  

---

## DBSCAN в R

Провести такую кластеризацию можно с помощью функции `dbscan` из пакета `dbscan`. 

```{r echo=FALSE}
circle_dbscan <- dbscan(multi, 0.23, 5)
fviz_cluster(circle_dbscan, data = multi,
             palette = my_col_circle)
```

---

## Epsilon --- выбираем расстояние для радиуса

Можно посмотреть на график k-расстояний (k-distance plot): поскольку мы предполагаем, что у точек одного кластера расстояние будет одинаковым, а у точек из шума расстояние будет больше. Вычислить среднее значение расстояний каждой точки до ее k ближайших соседей. Далее эти k-расстояния отображаются в порядке возрастания. Если на графике есть "колено" --- значительный перегиб, будет легко найти нужное значение радиуса. 
.pull-left[
```{r}
kNNdistplot(multi, k = 5)
abline(h = 0.23, lty = 2)
```
]
.pull-right[
```{r}
circle_dbscan <- dbscan(multi, 0.23, 5)
fviz_cluster(circle_dbscan, data = multi,
             palette = my_col_circle)
```
]

---

## Задание

Кластеризуйте данные по поссумам, используя DBSCAN-алгоритм.

---

## Примерное решение

.pull-left[
```{r purl=FALSE}
kNNdistplot(f_sc, k = 5)
abline(h = 2.3, lty = 2) 
```
]

.pull-right[
```{r purl=FALSE}
pos_db <- dbscan(f_sc, 2.3, 5)
fviz_cluster(pos_db, data = f_sc,
             palette = my_col)
```
]

Не очень красиво получилось. Нет близко расположенных точек, и многие наблюдения читаются как выбросы.

---

## Take home messages

- Неметрическое многомерное шкалирование (nMDS):
    - nMDS --- способ снижения размерности, сохраняющий ранги расстояний между объектами
    - Направления на графике многомерного шкалирования можно интерпретировать произвольным образом в зависимости от изменения других переменных (не обязательно вдоль осей)
    - Результат многомерного шкалирования зависит от выбора коэффициента различия
    - Стресс --- мера оценки качества ординации nMDS

- Кластерный анализ:
    - Бывает иерархическим и неиерархическим. Какой использовать --- зависит от ваших целей и задач
    - Результат кластеризации зависит не только от выбора коэффициента, но и от выбора алгоритма кластеризации
    - Кофенетическая корреляция --- мера оценки соответствия расстояний на дендрограмме и коэффициентов сходства/различия в исходной матрице

---

## Дополнительные ресурсы

- Borcard, D., Gillet, F., Legendre, P., 2011. Numerical ecology with R. Springer.
- Legendre, P., Legendre, L., 2012. Numerical ecology. Elsevier.
- Oksanen, J., 2011. Multivariate analysis of ecological communities in R: vegan tutorial. R package version 2–0.
- Quinn, G.G.P., Keough, M.J., 2002. Experimental design and data analysis for biologists. Cambridge University Press.

- Как работает UPGMA: http://www.southampton.ac.uk/~re1u06/teaching/upgma/

- pvclust: An R package for hierarchical clustering with p-values [WWW Document], n.d. URL http://www.sigmath.es.osaka-u.ac.jp/shimo-lab/prog/pvclust/ (accessed 11.7.14).

Для анализа филогенетических данных:

- Paradis, E., 2011. Analysis of Phylogenetics and Evolution with R. Springer.
- Список пакетов для филогенетического анализа в R:  
https://cran.r-project.org/web/views/Phylogenetics.html

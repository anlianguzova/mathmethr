---
title: "Регрессионный анализ, часть 1"
subtitle: "Математические методы в зоологии с использованием R"
author: "Марина Варфоломеева"
output:
  beamer_presentation:
    colortheme: seagull
    highlight: tango
    fonttheme: structurebold
    includes:
      in_header: ./includes/header.tex
    pandoc_args:
    - --latex-engine=xelatex
    - -V fontsize=10pt
    - -V lang=russian
    slide_level: 2
    theme: CambridgeUS
    toc: yes
---

```{r setup, include = FALSE, cache = FALSE, purl = FALSE}
options(width = 70, scipen = 6)
library(knitr)
opts_chunk$set(fig.show='hold', size='footnotesize', comment="#", warning=FALSE, message=FALSE, dev='cairo_pdf', fig.height=2.5, fig.width=7.7)
source("support_mathmethr.R")
```

### Вы сможете

- посчитать и протестировать различные коэффициенты корреляции между переменными
- подобрать модель линейной регрессии и записать ее в виде уравнения
- интерпретировать коэффициенты простой линейной регрессии
- протестировать значимость модели и ее коэффициентов при помощи t- или F-теста
- оценить долю изменчивости, которую объясняет модель, при помощи $R^2$

## Пример: потеря влаги личинками мучных хрущаков

\columnsbegin

\column{0.48\textwidth}

Как зависит потеря влаги личинками [малого мучного хрущака](http://ru.wikipedia.org/wiki/Хрущак_малый_мучной) _Tribolium confusum_ от влажности воздуха? 

- 9 экспериментов, продолжительность 6 дней
- разная относительная влажность воздуха, %
- измерена потеря влаги, мг

\column{0.48\textwidth}

\centering

\includegraphics[width=0.3\linewidth]{images/Tribolium_confusum.png}

\vfill
\tiny Малый мучной хрущак \textit{Tribolium confusum}, photo by Sarefo, CC BY-SA

\columnsend

\vfill
\tiny Nelson, 1964; данные из Sokal, Rohlf, 1997, табл. 14.1 по Logan, 2010. глава 8, пример 8c; Данные в файлах nelson.xlsx и nelson.csv


## Читаем данные из файла

### Чтение из xlsx
```{r}
library(readxl)
nelson <- read_excel(path = "data/nelson.xlsx", sheet = 1)
```

## Все ли правильно открылось?

```{r}
str(nelson)      # Структура данных
head(nelson)     # Первые несколько строк файла
```

## Знакомимся с данными

Есть ли пропущенные значения?

```{r}
colSums(is.na(nelson))
```

Каков объем выборки?

Поскольку пропущенных значений нет, можем просто посчитать число строк

```{r}
nrow(nelson)
```

Теперь все готово, чтобы мы могли ответить на вопрос, как зависит потеря веса от влажности?

# Графики средствами пакета ggplot2

## Грамматика графиков

1. Откуда брать данные?
2. Какие переменные изображать на графике?
3. В виде чего изображать?
4. Какие подписи нужны?
5. Какую тему оформления нужно использовать?

### Давайте поэтапно построим график

## С чего начинаются графики?

- `library(ggplot2)` --- активирует пакет ggplot2 со всеми его функциями
- `ggplot()` --- создает пустой "базовый" слой --- основу графика

```{r gg_base_1}
library(ggplot2)
ggplot()
```

## Откуда брать данные?

Обычно в основе графика пишут, откуда брать данные

```{r gg_base_2}
ggplot(data = nelson)
```

## Какие переменные изображать на графике?

Эстетики --- это свойства будущих элементов графика, которые будут изображать данные (`x`, `y`, `colour`, `fill`, `size`, `shape`, и т.д.)

`aes()` --- функция, которая сопоставляет значения эстетик и переменные из источника данных (название происходит от англ. _aesthetics_)

```{r gg_aes}
ggplot(data = nelson, aes(x = humidity, y = weightloss))
```

## В виде чего изображать?

Геомы --- графические элементы (`geom_point()`, `geom_line()`, `geom_bar()`, `geom_smooth()` и т.д., их очень много)

`geom_point()` --- точки

```{r gg-point}
ggplot(data = nelson, aes(x = humidity, y = weightloss)) + 
  geom_point()
```

## В виде чего изображать?

`geom_line()` --- линии

```{r, gg-line}
ggplot(data = nelson, aes(x = humidity, y = weightloss)) + 
  geom_line()
```

## Можно использовать несколько геомов одновременно

```{r, gg-point-line}
ggplot(data = nelson, aes(x = humidity, y = weightloss)) + 
  geom_point() + 
  geom_line()
```

## Подписи осей, заголовок и т.д.

Элемент `labs()` --- создает подписи. Аргументы --- это имена эстетик, например, `x`, `y` и т.д. Заголовок графика называется `title`


```{r gg-labs}
ggplot(data = nelson, aes(x = humidity, y = weightloss)) + 
  geom_point() + 
  labs(x = "Относительная влажность, %", y = "Потеря веса, мг",
       title = "Потеря веса мучных хрущаков \nпри разной влажности воздуха")
```

## Графики ggplot можно сохранять в переменные

```{r gg-var}
gg_nelson <- ggplot(data = nelson, aes(x = humidity, y = weightloss)) + 
  geom_point() + 
  labs(x = "Относительная влажность, %", y = "Потеря веса, мг")
gg_nelson
```

## Темы оформления графиков можно менять и настраивать

`theme()` --- меняет отдельные элементы (см. справку)
`theme_bw()`, `theme_classic()` и т.д. --- стили оформления целиком

```{r gg-themes}
gg_nelson + theme_classic()
```

## Можно установить любимую тему для всех последующих графиков

```{r gg-theme-set}
theme_set(theme_bw())
gg_nelson
```

## Графики можно сохранять в файлы

Функция `ggsave()` позволяет сохранять графики в виде файлов во множестве разных форматов ("eps", "ps", "tex", "pdf", "jpeg", "tiff", "png", "bmp", "svg" или "wmf"). Параметры изображений настраиваются (см. справку)

```{r gg-save, eval=FALSE}
ggsave(filename = "bugs_weightloss.png", plot = gg_nelson)
ggsave(filename = "bugs_weightloss.pdf", plot = gg_nelson)
```

# Корреляция

## Есть ли связь между переменными?

Судя по всему, да, скажем мы, глядя на график.

Но насколько сильна эта связь?

```{r}
gg_nelson
```


## Коэффициент корреляции --- способ оценки силы связи между двумя переменными

### Коэффициент корреляции Пирсона

- Оценивает только линейную составляющую связи
- Параметрические тесты значимости (t-критерий) применимы если переменные распределены нормально
  


В других случаях используются ранговые коэффициенты корреляции (например, кор. Кендалла и кор. Спирмена).


## Интерпретация коэффициента корреляции

\columnsbegin
\column{0.33\textwidth}
$$-1 < \rho < 1$$
\column{0.33\textwidth}
$|\rho| = 1$ --- сильная связь
\column{0.33\textwidth}
$\rho = 0$ --- нет связи
\columnsend

> - В тестах для проверки значимости тестируется гипотеза $H_0: \rho = 0$


```{r echo=FALSE, fig.height=4, out.height='4in', purl=FALSE}
#Title: An example of the correlation of x and y for various distributions of (x,y) pairs
#Tags: Mathematics; Statistics; Correlation
#Author: Denis Boigelot
#Packets needed : mvtnorm (rmvnorm), RSVGTipsDevice (devSVGTips)
#How to use: output()
#
#This is an translated version in R of an Matematica 6 code by Imagecreator.

library(mvtnorm)
# library(RSVGTipsDevice)

MyPlot <- function(xy, xlim = c(-4, 4), ylim = c(-4, 4), eps = 1e-15) {
   title = round(cor(xy[,1], xy[,2]), 1)
   if (sd(xy[,2]) < eps) title = "" # corr. coeff. is undefined
   plot(xy, main = title, xlab = "", ylab = "",
        col = "darkblue", pch = 16, cex = 0.2,
        xaxt = "n", yaxt = "n", bty = "n",
        xlim = xlim, ylim = ylim)
}

MvNormal <- function(n = 1000, cor = 0.8) {
   for (i in cor) {
      sd = matrix(c(1, i, i, 1), ncol = 2)
      x = rmvnorm(n, c(0, 0), sd)
      MyPlot(x)
   }
}

rotation <- function(t, X) return(X %*% matrix(c(cos(t), sin(t), -sin(t), cos(t)), ncol = 2))

RotNormal <- function(n = 1000, t = pi/2) {
   sd = matrix(c(1, 1, 1, 1), ncol = 2)
   x = rmvnorm(n, c(0, 0), sd)
   for (i in t)
      MyPlot(rotation(i, x))
}

Others <- function(n = 1000) {
   x = runif(n, -1, 1)
   y = 4 * (x^2 - 1/2)^2 + runif(n, -1, 1)/3
   MyPlot(cbind(x,y), xlim = c(-1, 1), ylim = c(-1/3, 1+1/3))

   y = runif(n, -1, 1)
   xy = rotation(-pi/8, cbind(x,y))
   lim = sqrt(2+sqrt(2)) / sqrt(2)
   MyPlot(xy, xlim = c(-lim, lim), ylim = c(-lim, lim))

   xy = rotation(-pi/8, xy)
   MyPlot(xy, xlim = c(-sqrt(2), sqrt(2)), ylim = c(-sqrt(2), sqrt(2)))
   
   y = 2*x^2 + runif(n, -1, 1)
   MyPlot(cbind(x,y), xlim = c(-1, 1), ylim = c(-1, 3))

   y = (x^2 + runif(n, 0, 1/2)) * sample(seq(-1, 1, 2), n, replace = TRUE)
   MyPlot(cbind(x,y), xlim = c(-1.5, 1.5), ylim = c(-1.5, 1.5))

   y = cos(x*pi) + rnorm(n, 0, 1/8)
   x = sin(x*pi) + rnorm(n, 0, 1/8)
   MyPlot(cbind(x,y), xlim = c(-1.5, 1.5), ylim = c(-1.5, 1.5))

   xy1 = rmvnorm(n/4, c( 3,  3))
   xy2 = rmvnorm(n/4, c(-3,  3))
   xy3 = rmvnorm(n/4, c(-3, -3))
   xy4 = rmvnorm(n/4, c( 3, -3))
   MyPlot(rbind(xy1, xy2, xy3, xy4), xlim = c(-3-4, 3+4), ylim = c(-3-4, 3+4))
}

output <- function() {
   # devSVGTips(width = 7, height = 3.2) # remove first and last line for no svg exporting
   par(mfrow = c(3, 7), oma = c(0,0,0,0), mar=c(2,2,2,0))
   MvNormal(800, c(1.0, 0.8, 0.4, 0.0, -0.4, -0.8, -1.0));
   RotNormal(200, c(0, pi/12, pi/6, pi/4, pi/2-pi/6, pi/2-pi/12, pi/2));
   Others(800)
   # dev.off() # remove first and last line for no svg exporting
}
output()
```

\vfill
\tiny
\href{https://commons.wikimedia.org/wiki/File\%3ACorrelation\\_examples2.svg}{By DenisBoigelot, original uploader was Imagecreator} [CC0], via Wikimedia Commons

## Можно расчитать значение коэффициента корреляции между потерей веса и влажностью

\fontsize{10pt}{10pt}

```{r}
p_cor <- cor.test(nelson$humidity, nelson$weightloss, 
         alternative = "two.sided", method = "pearson")
p_cor
```


Можно описать результаты несколькими способами:

- Величина потери веса мучных хрущаков отрицательно коррелирует с относительной влажностью воздуха ($r = `r round(p_cor$estimate, 2)`$, $p `r format.pval(p_cor$p.value, eps = 0.01)`$)
- Мучные хрущаки теряют вес при уменьшении относительной влажности воздуха ($r = `r round(p_cor$estimate, 2)`$, $p `r format.pval(p_cor$p.value, eps = 0.01)`$)

# Линейная регрессия


## Линейная регрессия

- позволяет описать функциональную зависимость между переменны
- позволяет предсказать значение одной переменной, зная значение другой

Зависимая переменная называется отклик

Те переменные, от которых она зависит --- предикторы

$$\hat y _i = b _0 + b _1 x _i$$

```{r echo=FALSE, purl=FALSE}
gg_nelson + geom_smooth(method = "lm")
```


## Линейная регрессия

- простая

$$Y _i = \beta _0 + \beta _1 x _i + \varepsilon _i$$

- множественная

$$Y _i = \beta _0 + \beta _1 x _{1 i} + \beta _2 x _{2 i} + ... + \varepsilon _i$$

## Интерпретация коэффициентов регрессии

$$\hat y _i = b _0 + b _1 x _i$$

\includegraphics[width=\linewidth]{images/interpretation-of-regression-coefficients.png}

\tiny Рисунок из кн. Logan, 2010, стр. 170, рис. 8.2
\normalsize

\vfill

- $b_0$ --- Отрезок (Intercept), отсекаемый регрессионной прямой на оси $y$. Значение зависимой переменной $y$, если предиктор $x = 0$.
- $b_1$ --- Коэффициент угла наклона регрессионной прямой. Показывает на сколько единиц изменяется отклик ($y$), при увеличении значения предиктора ($x$) на единицу.

# Подбор коэффициентов линейной регрессии

## Как провести линию регрессии?

$$\hat y _i = \beta _0 + \beta _1 x _i + \varepsilon_i$$

```{r echo=FALSE}
gg_nelson + 
  geom_abline(slope = coef(nelson_lm)[2], 
              intercept = coef(nelson_lm)[1], 
              colour = "steelblue3", size = 1) +
  geom_abline(slope = -0.058, intercept = 8.8, 
              colour = "orange3", size = 1) +
  geom_abline(slope = -0.05, intercept = 8.9,
              colour = "green3", size = 1)
```

Нужно оценить параметры линейной модели:

- $\beta _0$
- $\beta _1$

Но как это сделать?

## Метод наименьших квадратов --- один из способов подбора параметров

$$\hat y _i = b _0 + b _1 x _i$$

\columnsbegin

\column{0.4\textwidth}

Нужно получить оценки параметров линейной модели:

- $b _0$
- $b _1$

Оценки параметров линейной регрессии подбирают так, чтобы минимизировать остатки $\sum{(y _i - \hat y _i)^2}$, т.е. $\sum{\varepsilon^2_i}$


\column{0.6\textwidth}

\includegraphics[width=0.9\linewidth]{images/OLS-regression-line.png}

Линия регрессии по методу наименьших квадратов

\vfill
\tiny из кн. Quinn, Keough, 2002, стр. 85, рис. 5.6 a

\columnsend

## Оценки параметров линейной регрессии

\begin{tabular}{l l l}
 \hline\noalign{\smallskip}
Параметры & Оценки параметров & Стандартные ошибки оценок \\
 \hline\noalign{\smallskip}
$\beta _1$    & $b _1 = \frac {\sum _{i=1}^{n} {[(x _i - \bar x)(y _i - \bar y)]}}{\sum _{i=1}^{n} {(x _i - \bar x)^2}}$      & $SE _{b _1} = \sqrt{\frac{MS _e}{\sum _{i=1}^{n} {(x _i - \bar x)^2}}}$ \\
$\beta _0$    & $b _0 = \bar y - b _1 \bar x$  & $SE _{b _0} = \sqrt{MS _e [\frac{1}{n} + \frac{\bar x}{\sum _{i=1}^{n} {(x _i - \bar x)^2}}]}$ \\
 \hline\noalign{\smallskip}
\end{tabular}

\tiny Таблица из кн. Quinn, Keough, 2002, стр. 86, табл. 5.2
\normalsize

Стандартные ошибки коэффициентов
  - используются для построения доверительных интервалов
  - нужны для статистических тестов

### Доверительный интервал коэффициента

  - зона, в которой с $(1 - \alpha) \cdot 100\%$ вероятностью содержится среднее значение коэффициента
  - $b _1 \pm t _{\alpha, df = n - 2} \cdot SE _{b _1}$
  - $\alpha = 0.05$ => $(1 - 0.05) \cdot 100\% = 95\%$ интервал

## Доверительная зона регрессии

```{r, nelson-conf, echo=FALSE, purl=FALSE}
gg_nelson + geom_smooth(method = "lm")
```

### Доверительная зона регрессии

- $(1 - \alpha) \cdot 100\%$ доверительная зона регрессии
- зона, в которой с $(1 - \alpha) \cdot 100\%$ вероятностью лежит регрессионная прямая
- Возникает из-за неопределенности оценок коэффициентов регрессии

## Неопределенность оценок предсказанных значений

### Доверительный интервал к предсказанному значению
  - зона в которую попадают $(1 - \alpha) \cdot 100\%$ значений $\hat y _i$ при данном $x _i$
  - $\hat y _i \pm t _{\alpha, n - 2} \cdot SE _{\hat y _i}$
  - $SE _{\hat y} = \sqrt{MS _{e} [1 + \frac{1}{n} + \frac{(x _{prediction} - \bar x)^2} {\sum _{i=1}^{n} {(x _{i} - \bar x)^2}}]}$

### Доверительная область значений регрессии
  - зона, в которую попадает $(1 - \alpha) \cdot 100\%$ всех предсказанных значений

```{r, nelson-pr-all, echo=FALSE, results='hide', purl=FALSE}
(pr_all <- predict(nelson_lm, interval = "prediction"))
nelson_with_pred <- data.frame(nelson, pr_all)
```

```{r, nelson-pred, echo=FALSE, fig.height=1.75, purl=FALSE}
gg_nelson + geom_smooth(method = "lm", se = FALSE) +
  geom_ribbon(data = nelson_with_pred, 
              aes(y = fit, ymin = lwr, ymax = upr), 
              fill = 'green', alpha = 0.2)
```

<!-- ## Стандартизованные коэффициенты -->

<!-- Стандартизованные коэффициенты используются для сравнения вкладов предикторов в изменение отклика. Их можно использовать при сравнении разных моделей. -->

<!-- - Не зависят от масштаба измерений x и y -->
<!-- - Можно вычислить, зная обычные коэффициенты и их стандартные отклонения $b^\ast _1 = {b _1  \frac {\sigma _x} {\sigma _y}}$ -->
<!-- - Можно вычислить, посчитав регрессию по стандартизованным данным -->

# Линейная регрессия в R

## Как в R задать формулу линейной регрессии

`lm(формула, данные)` - функция для подбора регрессионных моделей

Формат формулы: `зависимая_переменная ~ модель`

- $\hat y _i = b _0 + b _1 x _i$ (простая линейная регрессия с $b _0$ (intercept))
    - Y ~ X
    - Y ~ 1 + X 
    - Y ~ X + 1

- $\hat y _i = b _1 x _i$ (простая линейная регрессия без $b _0$)
    - Y ~ X - 1
    - Y ~ -1 + X

- $\hat y _i = b _0$ (уменьшенная модель, линейная регрессия Y от $b _0$)
    - Y ~ 1
    - Y ~ 1 - X

## Примеры формул линейной регрессии

- $\hat y _i = b _0 + b _1 x _{1 i} + b _2 x _{2 i} + b _3 x _{3 i}$

(множественная линейная регрессия с $b _0$)

    Y ~ X1 + X2 + X3
    
    Y ~ 1 + X1 + X2 + X3

- $\hat y _i = b _0 + b _1 x _{1 i} + b _3 x _{3 i}$

(уменьшенная модель множественной линейной регрессии, без $x _2$)

    Y ~ X1 + X3
    
    Y ~ 1 + X1 + X3

## Подбираем параметры линейной модели

\fontsize{10pt}{10pt}

```{r, nelson-reg}
nelson_lm <- lm(weightloss ~ humidity, nelson)
summary(nelson_lm)
```


Коэффициенты линейной регрессии:

- $b _0 =  `r format(coef(nelson_lm)[1], digits = 2)` \pm `r format(coef(summary(nelson_lm))[1, 2], digits = 1)`$
- $b _1 =  `r format(coef(nelson_lm)[2], digits = 2)` \pm `r format(coef(summary(nelson_lm))[2, 2], digits = 1)`$

## Записываем уравнение линейной регрессии

Коэффициенты модели:

```{r}
coef(nelson_lm)
```

Уравнение регрессии:  

```{r results='asis', echo=FALSE, purl=FALSE}
lm_equation(nelson_lm, strict=FALSE, digits = 1)
```

Более формальная запись:  

```{r results='asis', echo=FALSE, purl=FALSE}
lm_equation(nelson_lm, digits = 1)
```

# Тестирование значимости модели и ее коэффициентов

## Способы проверки значимости модели и ее коэффициентов

Существует несколько способов проверки значимости модели

- Значима ли модель целиком?
    + F критерий: действительно ли объясненная моделью изменчивость больше, чем остаточная изменчивость

- Значима ли связь между предиктором и откликом?
    + t-критерий: отличается ли от нуля коэффициент при этом предикторе
    + F-критерий: действительно ли объясненная предиктором изменчивость больше, чем случайная?

## Тестируем значимость коэффициентов t-критерием

### t-критерий

$$t = \frac{b _1 - \theta}{SE _{b _1}}$$

$H _0 : b _1 = \theta$, для $\theta = 0$

Число степеней свободы $df = n - 2$

## Тестируем значимость коэффициентов с помощью t-критерия

\fontsize{10pt}{10pt}

```{r}
summary(nelson_lm)
```


Результаты можно описать в тексте так:

- Увеличение относительной влажности привело к достоверному замедлению потери веса жуками ($b _1 = -0.053$, $t = - 16.35$, $p < 0.01$)

## Тестируем значимость модели целиком при помощи F-критерия

### F-критерий

$$F = {MS _{regression} \over MS _{error}}$$

$H _0: \beta _1 = 0$ 

Число степеней свободы $df _{regression}$, $df _{error}$

## Общая изменчивость

Общая изменчивость --- $SS _{total}$, сумма квадратов отклонений от общего среднего значения

\centering
\includegraphics[width=.5\textwidth]{images/total-variation.png}

\raggedright
\vfill
\tiny Рис. из кн. Logan, 2010, стр. 172, рис. 8.3


## Общая изменчивость делится на объясненную и остаточную

$SS _{total} = SS _{regression} + SS _{error}$

\columnsbegin
\column{0.48\linewidth}
\centering
\includegraphics[width=.9\textwidth]{images/explained-variation.png}

Объясненная изменчивость

\column{0.48\linewidth}
\centering
\includegraphics[width=.9\textwidth]{images/residual-variation.png}

Остаточная изменчивость

\columnsend

\vfill
\tiny Рис. из кн. Logan, 2010, стр. 172, рис. 8.3

## Если зависимости нет, $b _1 = 0$

Тогда $\hat y _i = \bar y _i$ и $MS _{regression} \approx MS _{error}$

\columnsbegin
\column{0.48\linewidth}
\centering
\includegraphics[width=.9\textwidth]{images/explained-variation.png}

Объясненная изменчивость

\column{0.48\linewidth}
\centering
\includegraphics[width=.9\textwidth]{images/residual-variation.png}

Остаточная изменчивость

\columnsend

\vfill
\tiny Рис. из кн. Logan, 2010, стр. 172, рис. 8.3

<!-- ## Что оценивают средние квадраты отклонений? -->

<!-- \resizebox{1\textwidth}{!}{ -->
<!-- \begin{tabular}{ L{2.3cm} C{1.5cm} C{2cm} C{2.5cm} C{4cm}} -->
<!-- \hline\noalign{\smallskip} -->
<!-- Источник \linebreak[4] изменчивости &  Число степеней свободы df & Суммы квадратов отклонений SS & Средний квадрат отклонений MS & Ожидаемый средний квадрат \\ -->
<!-- \hline\noalign{\smallskip} -->
<!-- Регрессия & $1$ & $\sum{(\bar y - \hat y _i)^2}$ & $\frac{\sum _{i=1}^{n}{(\bar y - \hat y _i)^2}}{1}$ & $\sigma _{\varepsilon} ^2 + {\beta _1} ^2 \sum _{i=1}^{n} {(x _i - \bar x)^2}$ \\ -->
<!-- Остаточная & $n - 2$ & $\sum{(y _i - \hat y _i)^2}$ & $\frac{\sum _{i=1}^{n}{(y _i - \hat y _i)^2}}{n - 2}$ & $\sigma _{\varepsilon} ^2$ \\ -->
<!-- Общая & $n - 1$ & $\sum {(\bar y - y _i)^2}$ &  & \\ -->
<!-- \hline\noalign{\smallskip} -->
<!-- \end{tabular} -->
<!-- } -->


<!-- Если $b _1 = 0$, тогда $\hat y _i = \bar y _i$ и $MS _{r} \approx MS _{e}$ -->

<!-- Тестируем: -->

<!-- $$F = {MS _{regression} \over MS _{error}}$$ -->

## F-критерий и распределение F-статистики

Если $b _1 = 0$, тогда $\hat y _i = \bar y _i$ и $MS _{r} \approx MS _{e}$

\columnsbegin
\column{0.38\linewidth}
F - соотношение объясненной и не объясненной изменчивости
$$F = {MS _{regression} \over MS _{error}}$$
Зависит от

- $\alpha$
- $df _{r}$
- $df _{e}$


\column{0.58\linewidth}
\centering
\includegraphics[width=.9\textwidth]{images/f-distribution.png}

Распределение F-статистики при справедливой $H _0$

\vfill
\tiny Рис. из кн. Logan, 2010, стр. 172, рис. 8.3

\columnsend


## Таблица результатов дисперсионного анализа

\resizebox{1\textwidth}{!}{
\begin{tabular}{L{2.2cm} c c c c}
\hline\noalign{\smallskip}
Источник \linebreak[2] изменчивости  & df & SS & MS & F  \\
\hline\noalign{\smallskip}
Регрессия & $df _r = 1$ & $SS _r = \sum{(\bar y - \hat y _i)^2}$ & $MS _r = \frac{SS _r}{df _r}$ & $F _{df _r, df _e} = \frac{MS _r}{MS _e}$ \\
Остаточная & $df _e = n - 2$ & $SS _e = \sum{(y _i - \hat y _i)^2}$ & $MS _e = \frac{SS _e}{df _e}$ \\
Общая & $df _t = n - 1$ & $SS _t = \sum {(\bar y - y _i)^2}$ & & \\
\hline\noalign{\smallskip}
\end{tabular}
}

\large Минимальное упоминание результатов в тексте должно содержать $F _{df _r, df _e}$ и $p$.

## Проверяем значимость модели при помощи F-критерия

```{r}
library(car)
nelson_aov <- Anova(nelson_lm, type = 3)
summary(nelson_aov)
```

Результаты дисперсионного анализа можно описать в тексте (или представить в виде таблицы):

- Количество влаги, потерянной жуками в период эксперимента, достоверно зависело от уровня относительной влажности \linebreak[2] ($F _{1, 7} = 267$, $p < 0.01$).

# График линейной регрессии

## Строим доверительную зону регрессии

```{r eval=FALSE}
gg_nelson + geom_smooth(method = "lm") + 
  labs (title = "95% доверительная зона регрессии")

gg_nelson + geom_smooth(method = "lm", level = 0.99) + 
  labs (title = "99% доверительная зона регрессии")
```

```{r echo=FALSE, purl=FALSE}
library(gridExtra)
grid.arrange(
gg_nelson + geom_smooth(method = "lm") + 
  labs (title = "95% доверительная зона регрессии"),
gg_nelson + geom_smooth(method = "lm", level = 0.99) + 
  labs (title = "99% доверительная зона регрессии"),
ncol = 2)
```

# Оценка качества подгонки модели

## Коэффициент детерминации

### Коэффициент детерминации $R^2$

доля общей изменчивости, объясненная линейной связью x и y

$$R^2 =\frac{SS_{r}}{SS_{t}} = 1 - \frac{SS_{e}}{SS_{t}}$$

$$0 \le R^2 \le 1$$

Иначе рассчитывается как квадрат коэффициента корреляции $R^2 = r^2$

__Не используйте обычный $R^2$ для множественной регрессии!__


## Коэффициент детерминации можно найти в сводке модели

\fontsize{10pt}{10pt}
```{r}
summary(nelson_lm)
```

## Сравнение качества подгонки моделей

### $R^2_{adj}$ --- cкорректированный $R^2$

$$R^2_{adj} = 1 - \frac{SS_{e} / df_{e}}{SS_{t} / df_{t}}$$

где $df_{e} = n - p - 1$, $df_{t} = n - 1$

$R^2_{adj}$ учитывает число переменных в модели, вводится штраф за каждый новый параметр.

Используйте $R^2 _{adj}$ для сравнения моделей с разным числом параметров.

# Использование линейной регрессии для предсказаний (для самостоятельного разбора)

## Использование линейной регрессии для предсказаний

Для конкретного значения предиктора мы можем сделать два типа предсказаний:

- предсказываем среднее значение отклика --- это оценка точности положения линии регрессии
- предсказываем значение отклика у 95% наблюдений --- это оценка точности предсказаний

## Предсказываем Y при заданном X 

Какова средняя потеря веса при заданной влажности?

\fontsize{10pt}{10pt}

```{r fig.height = 7}
newdata <- data.frame(humidity = c(50, 100)) # значения, для которых предсказываем
(pr1 <- predict(nelson_lm, newdata, interval = "confidence", se = TRUE))
```


- При 50 и 100\% относительной влажности ожидаемая средняя потеря веса жуков будет `r round(pr1$fit[1,1], 1)` $\pm$ `r round(pr1$fit[1,1] - pr1$fit[1,2], 1)` и `r round(pr1$fit[2,1], 1)` $\pm$ `r round(pr1$fit[2,1] - pr1$fit[2,2], 1)`, соответственно.

## Предсказываем изменение Y для 95\% наблюдений при заданном X

В каких пределах находится потеря веса у 95\% жуков при заданной влажности?

\fontsize{10pt}{10pt}

```{r}
newdata <- data.frame(humidity = c(50, 100)) # новые данные для предсказания значений
(pr2 <- predict(nelson_lm, newdata, interval = "prediction", se = TRUE))
```

- У 95\% жуков при 50 и 100\% относительной влажности будет потеря веса будет в пределах `r round(pr2$fit[1,1], 1)` $\pm$ `r round(pr2$fit[1,1] - pr2$fit[1,2], 1)` и `r round(pr2$fit[2,1], 1)` $\pm$ `r round(pr2$fit[2,1] - pr2$fit[2,2], 1)`, соответственно.

## Данные для доверительной области значений

Предсказанные значения для исходных данных объединим с исходными данными в новом датафрейме - для графиков

```{r, nelson-pr-all}
```


## Строим доверительную область значений и доверительный интервал одновременно

```{r, nelson-plot-all}
gg_nelson + 
  geom_smooth(method = "lm", 
              aes(fill = "Доверительный \nинтервал"), 
              alpha = 0.4) +
  geom_ribbon(data = nelson_with_pred, 
              aes(y = fit, ymin = lwr, ymax = upr, 
                  fill = "Доверительная \nобласть значений"), 
              alpha = 0.2) +
  scale_fill_manual('Интервалы', values = c('green', 'blue'))
```

## Осторожно!

### Вне интервала значений $X$ ничего предсказать нельзя!
```{r echo=FALSE, fig.height=3, out.height='3in', purl=FALSE}
gg_nelson + 
  geom_smooth(method = "lm", 
              aes(fill = "Доверительный \nинтервал"), 
              alpha = 0.4) +
  geom_ribbon(data = nelson_with_pred, 
              aes(y = fit, ymin = lwr, ymax = upr, 
                  fill = "Доверительная \nобласть значений"), 
              alpha = 0.2) +
  scale_fill_manual('Интервалы', values = c('green', 'blue')) + xlim(-10, 110)
```



## Take home messages

- Модель простой линейной регрессии $y _i = \beta _0 + \beta _1 x _i + \varepsilon _i$
- В оценке коэффициентов регрессии и предсказанных значений существует неопределенность. Доверительные интервалы можно расчитать, зная стандартные ошибки.
- Значимость всей регрессии и ее параметров можно проверить при помощи t- или F-теста. $H _0: \beta _1 = 0$
- Качество подгонки модели можно оценить при помощи коэффициента детерминации $R^2$

## Дополнительные ресурсы

- Гланц, 1999, стр. 221-244
- OpenIntro: Statistics
- Quinn, Keough, 2002, pp. 78-110
- Logan, 2010, pp. 170-207
- Sokal, Rohlf, 1995, pp. 451-491
- Zar, 1999, pp. 328-355

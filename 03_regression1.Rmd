---
title: "Регрессионный анализ, часть 1"
subtitle: "Математические модели в зоологии"
author: 
  - Марина Варфоломеева
  - Анастасия Лянгузова
company: 'Каф. Зоологии беспозвоночных, СПбГУ'
output:
  xaringan::moon_reader:
    self-contained: true
    lib_dir: libs
    css: [ninjutsu, "assets/xaringan-themer.css", "assets/xaringan.css"]
    df_print: default
    nature:
      highlightStyle: vs
      highlightLines: true
      countIncrementalSlides: false
      titleSlideClass: [middle, left, inverse]
      beforeInit: "assets/cols_macro.js"
    includes:
      in_header: "assets/xaringan_in_header.html"
      after_body: "assets/xaringan_after_body.html"
---

```{r setup, include = FALSE, cache = FALSE, purl = TRUE}
source("assets/xaringan_setup.R")
library(xaringanExtra)
use_tile_view()
use_scribble()
use_search(show_icon = FALSE)
use_progress_bar(color = "#98BF64", location = "bottom", height = "10px")
use_freezeframe()
# use_webcam()
# use_panelset()
# use_extra_styles(hover_code_line = TRUE)

# http://tachyons.io/docs/
# https://roperzh.github.io/tachyons-cheatsheet/
use_tachyons()
source('support_mathmethr.R')
```

## Вы сможете

- посчитать и протестировать различные коэффициенты корреляции между переменными
- подобрать модель линейной регрессии и записать ее в виде уравнения
- интерпретировать коэффициенты простой линейной регрессии
- протестировать значимость модели и ее коэффициентов при помощи t- или F-теста
- оценить долю изменчивости, которую объясняет модель, при помощи $R^2$

---

## Пример: возраст мёртвыx людей

После испытаний ядерной бомбы в 1955--1963 годаx резко возросло количество нестабильного изотопа углерода $^{14}C$. Один из методов определения возраста умершего человека основан на изерении содержания $^{14}C$ в эмали. Проверим, насколько этот метод точен, и насколько сильна связь между следующими параметрами.

.pull-left[

- `dateOfBirth` ---
год рождения человека;
- `deltaC14` ---
содержание изотопа $^{14}C$ относительно нормального (до ядерного времени, в %).
]

.pull-right[
![](images/sled_scull.jpg)]

.tiny[Spalding et al. 2005; данные из Whitlock, Schluter, 2015, глава 17, упр. 31; Данные в файлах nuclear_teeth.xlsx и nuclear_teeth.csv]

---

## Читаем данные из файла

Чтение из xlsx:

```{r}
library(readxl)
teeth <- as.data.frame(read_excel(path = 'data/nuclear_teeth.xlsx', sheet = 1))
```


Чтение из csv:

```{r}
teeth <- read.table(file = 'data/nuclear_teeth.csv', header = TRUE, sep = ',')
```

---

## Все ли правильно открылось?

```{r}
str(teeth)      # Структура данных
head(teeth, 3)     # Первые 3 строки файла
```

---

## Сделаем более короткие имена

Сейчас переменные называются так:

```{r}
colnames(teeth)
```

Сделаем более удобные названия:

```{r}
colnames(teeth) <- c('birth', 'c14')
```

Теперь переменные стали называться так:

```{r}
colnames(teeth)
```

---

## Знакомимся с данными

Есть ли пропущенные значения?

```{r}
colSums(is.na(teeth))
```

Каков объем выборки?

Поскольку пропущенных значений нет, можем просто посчитать число строк:

```{r}
nrow(teeth)
```

Теперь все готово, чтобы мы могли ответить на вопрос исследования.

---

class: middle, center, inverse

# Корреляция

---

## Есть ли связь между переменными?

Построим график!

```{r, purl=FALSE}
library(ggplot2)
gg_teeth <- ggplot(data = teeth, aes(x = c14, y = birth)) + 
  geom_point() + 
  labs(x = 'Содержание нестабильного изотопа углерода, %', 
       y = 'Год рождения человека')
gg_teeth
```

Судя по всему, да. 

Но насколько сильна эта связь?

---

## Коэффициент корреляции --- способ оценки силы связи между двумя переменными

__Коэффициент корреляции Пирсона__

- Оценивает только линейную составляющую связи
- Параметрические тесты значимости такие как t-тест применимы, если переменные распределены нормально.

| Параметрические статистики | Непараметрические статистики |
| -------------------------- | ---------------------------- |
| Работают только с нормально распределёнными величинами | Работают с любыми формами распределения |
| Имеют большую мощность | Имеют меньшую мощность |
| Могут работать с группами, имеющими разную дисперсию | Не могут работать с группами, имеющими разную дисперсию |
| Анализируют исключительно непрерывные данные | Могут работать с ранговыми данными |
| t-тест, one-way ANOVA, коэффициент корреляции Пирсона | Тест Манна-Уитни, коэффициент корреляции Спирмена или Кендалла | 

---

## Интерпретация коэффициента корреляции

| $-1 < \rho < 1$ | $abs(\rho) = 1$ | $\rho = 0$ |
| --------------- | -------------| ------------------ |
| | сильная связь |  нет связи |

В тестах для проверки значимости тестируется гипотеза $H_0: \rho = 0$.


```{r inter-coeff-cor, echo=FALSE, purl=FALSE, out.width='70%'}
#Title: An example of the correlation of x and y for various distributions of (x,y) pairs
#Tags: Mathematics; Statistics; Correlation
#Author: Denis Boigelot
#Packets needed : mvtnorm (rmvnorm), RSVGTipsDevice (devSVGTips)
#How to use: output()
#
#This is an translated version in R of an Matematica 6 code by Imagecreator.

library(mvtnorm)
# library(RSVGTipsDevice)

MyPlot <- function(xy, xlim = c(-4, 4), ylim = c(-4, 4), eps = 1e-15) {
   title = round(cor(xy[,1], xy[,2]), 1)
   if (sd(xy[,2]) < eps) title = '' # corr. coeff. is undefined
   plot(xy, main = title, xlab = '', ylab = '',
        col = 'darkblue', pch = 16, cex = 0.2,
        xaxt = 'n', yaxt = 'n', bty = 'n',
        xlim = xlim, ylim = ylim)
}

MvNormal <- function(n = 1000, cor = 0.8) {
   for (i in cor) {
      sd = matrix(c(1, i, i, 1), ncol = 2)
      x = rmvnorm(n, c(0, 0), sd)
      MyPlot(x)
   }
}

rotation <- function(t, X) return(X %*% matrix(c(cos(t), sin(t), -sin(t), cos(t)), ncol = 2))

RotNormal <- function(n = 1000, t = pi/2) {
   sd = matrix(c(1, 1, 1, 1), ncol = 2)
   x = rmvnorm(n, c(0, 0), sd)
   for (i in t)
      MyPlot(rotation(i, x))
}

Others <- function(n = 1000) {
   x = runif(n, -1, 1)
   y = 4 * (x^2 - 1/2)^2 + runif(n, -1, 1)/3
   MyPlot(cbind(x,y), xlim = c(-1, 1), ylim = c(-1/3, 1+1/3))

   y = runif(n, -1, 1)
   xy = rotation(-pi/8, cbind(x,y))
   lim = sqrt(2+sqrt(2)) / sqrt(2)
   MyPlot(xy, xlim = c(-lim, lim), ylim = c(-lim, lim))

   xy = rotation(-pi/8, xy)
   MyPlot(xy, xlim = c(-sqrt(2), sqrt(2)), ylim = c(-sqrt(2), sqrt(2)))
   
   y = 2*x^2 + runif(n, -1, 1)
   MyPlot(cbind(x,y), xlim = c(-1, 1), ylim = c(-1, 3))

   y = (x^2 + runif(n, 0, 1/2)) * sample(seq(-1, 1, 2), n, replace = TRUE)
   MyPlot(cbind(x,y), xlim = c(-1.5, 1.5), ylim = c(-1.5, 1.5))

   y = cos(x*pi) + rnorm(n, 0, 1/8)
   x = sin(x*pi) + rnorm(n, 0, 1/8)
   MyPlot(cbind(x,y), xlim = c(-1.5, 1.5), ylim = c(-1.5, 1.5))

   xy1 = rmvnorm(n/4, c( 3,  3))
   xy2 = rmvnorm(n/4, c(-3,  3))
   xy3 = rmvnorm(n/4, c(-3, -3))
   xy4 = rmvnorm(n/4, c( 3, -3))
   MyPlot(rbind(xy1, xy2, xy3, xy4), xlim = c(-3-4, 3+4), ylim = c(-3-4, 3+4))
}

output <- function() {
   # devSVGTips(width = 7, height = 3.2) # remove first and last line for no svg exporting
   par(mfrow = c(3, 7), oma = c(0,0,0,0), mar=c(2,2,2,0))
   MvNormal(800, c(1.0, 0.8, 0.4, 0.0, -0.4, -0.8, -1.0));
   RotNormal(200, c(0, pi/12, pi/6, pi/4, pi/2-pi/6, pi/2-pi/12, pi/2));
   Others(800)
   # dev.off() # remove first and last line for no svg exporting
}
output()
```

.tiny[
[By DenisBoigelot, original uploader was Imagecreator](https://commons.wikimedia.org/wiki/File\%3ACorrelation\\_examples2.svg) [CC0], via Wikimedia Commons
]

---

## Задание 1

Дополните код, чтобы вычислить корреляцию Пирсона между годом рождения и количеством нестабильного изотопа. 

Используйте нужные переменные из датасета `teeth` и функцию `cor.test()`

```
p_cor <- cor.test(x = , y = , 
         alternative =  , method =  )
p_cor
```

---

## Решение: корреляция между годом рождения и количеством нестабильного изотопа


```{r, purl=FALSE}
p_cor <- cor.test(x = teeth$c14, y = teeth$birth, 
         alternative = 'two.sided', method = 'pearson')
p_cor
```


Можно описать результаты несколькими способами:

- Год рождения отрицательно коррелирует с содержанием изотопа $^{14}C$ $(r =$ `r round(p_cor$estimate, 2)`, $p =$ `r format.pval(p_cor$p.value, eps = 0.01)`). 
- Изотопа $^{14}C$ становится меньше с увеличением года рождения $(r =$ `r round(p_cor$estimate, 2)`, $p =$ `r format.pval(p_cor$p.value, eps = 0.01)`).

Т.е. чем младше человек, тем меньше в его эмали радиоактивного углерода. 

---

class: middle, center, inverse

# Линейная регрессия

---

## Линейная регрессия

- позволяет описать зависимость между количественными величинами;
- позволяет предсказать значение одной величины, зная значения других (что супер полезно в случае нашего датасета! и для криминалистов).

$$y _i = \beta _0 + \beta _1 x _{1i} + \varepsilon_i$$

```{r echo=FALSE, purl=FALSE}
gg_teeth + geom_smooth(method = 'lm')
```


---

## Линейная регрессия бывает простая и множественная

- простая

$$y _i = \beta _0 + \beta _1 x _i + \varepsilon _i$$

- множественная

$$y _i = \beta _0 + \beta _1 x _{1 i} + \beta _2 x _{2 i} + ... + \varepsilon _i$$

---

## Детерминистские и стохастические модели

.pull-left[
```{r determ-model, echo=FALSE, fig.height=4, fig.width=4, warning=FALSE, purl=FALSE}
x <- 1:20
y <- 2 + 5*x
ggplot(data.frame(x=x, y=y), aes(x=x, y=y)) + geom_point(size=4)  + geom_abline(slope=5, intercept = 2) + ylim(0, 100) 
```

Модель: $y _i = \beta _0 + \beta _1 x _i$    
]

.pull-right[
```{r stoh-model, echo=FALSE, fig.height=4, fig.width=4, warning=FALSE, purl=FALSE}
x <- 1:20
y <- 2 + 5*x + rnorm(20,0, 20)
ggplot(data.frame(x=x, y=y), aes(x=x, y=y)) + geom_point(size=4)  + geom_abline(slope=5, intercept = 2)  + ylim(0,100) + theme_bw()

```

Модель: $у_ i = \beta _0 + \beta _1 x _i + \varepsilon_i$    

Появляется дополнительный член $\varepsilon_i$, который вводит в модель влияние неучтенных моделью факторов. 
Обычно считают, что $\epsilon \in N(0, \sigma^2)$ 
]

---

## Линейная регрессия в генеральной совокупности и в выборке

В уравнении линейной регрессии, описывающей зависимость в  генеральной совокупности, обозначения записываются греческими буквами:

$$y _i = \beta _0 + \beta _1 x _{1i} + \varepsilon_i$$

Обозначения в уравнении модели, построенной по выборке --- латинскими:

$$y _i = b _0 + b _1 x _i + e_i$$

---

## Что есть что в уравнении линейной регрессии

.pull-left[

```{r echo=FALSE, purl=FALSE, fig.height=6, fig.width=5}
dfr <- data.frame(x = c(-1,  -0.5, 0.8, 2.5, 3, 4), 
                  y = c(-0.5, 1.5, 1,   5,   2, 3.5))
mod <- lm(y ~ x, data = dfr)
cf <- coef(mod)

gg_coefs <- ggplot(data = dfr, aes(x = x, y = y)) +
  geom_point(colour = 'steelblue') +
  coord_equal(xlim = c(-2, 5), ylim = c(-1, 6), expand = FALSE) +
  scale_x_continuous(breaks = -1:5) + scale_y_continuous(breaks = -1:6) + 
  geom_hline(yintercept = 0) + geom_vline(xintercept = 0) +
  geom_smooth(method = 'lm', se = FALSE) +
  # остаток
  annotate('segment', x = 2.5, y = 5, xend = 2.5, yend = predict(mod, data.frame(x = 2.5)), linetype = 'dashed', size = 0.8) + 
  annotate('text', label = 'e[i] == y[i] - hat(y)[i]', x = 2.7, y = 4.5, parse = T, size = 4, hjust = 0) +
  # предсказанное
  annotate('text', label = 'y[i]', x = 2, y = 5, parse = T, size = 4) +
  annotate('point', x = 2.5, y = 5, size = 3, colour = 'red3') +
  # наблюдаемое
  annotate('text', label = 'hat(y)[i]', x = 2, y = 3, parse = T, size = 4) +
  # x_i
  annotate('segment', x = 2.5, y = 0, xend = 2.5, yend = predict(mod, data.frame(x = 2.5)), linetype = 'dotted') +
  annotate('text', label = 'x[i]', x = 2.5, y = -0.4, parse = T, size = 4) +
  # отрезок
  annotate('segment', x = 0, y = cf[1], xend = 0, yend = 0, size = 2, colour = 'red3')  +
  annotate('text', label = 'b[0]', x = -0.4, y = 0.3, parse = T, size = 4) +
  # угол наклона
  annotate('segment', x = 1, y = 0, xend = 1, yend = predict(mod, data.frame(x = 1)), linetype = 'dotted') +
  annotate('segment', x = 2, y = 0, xend = 2, yend = predict(mod, data.frame(x = 2)), linetype = 'dotted') +
  annotate('segment', x = 1, y = predict(mod, data.frame(x = 1)), xend = 0, yend = predict(mod, data.frame(x = 1)), linetype = 'dotted') +
  annotate('segment', x = 2, y = predict(mod, data.frame(x = 2)), xend = 0, yend = predict(mod, data.frame(x = 2)), linetype = 'dotted') +
  annotate('segment', x = 0, y = predict(mod, data.frame(x = 1)), xend = 0, yend = predict(mod, data.frame(x = 2)), size = 2, colour = 'orange2') +
  annotate('text', label = 'b[1]', x = -0.4, y = 2, parse = T, size = 4) +
  annotate('segment', x = 1, y = 0, xend = 2, yend = 0, size = 1) +
  annotate('text', label = '1', x = 1.5, y = -0.4, size = 4) +
  theme(text = element_text(size = 18))
  
gg_coefs
```
]

.pull-right[

$$y _i = b _0 + b _1 x _i + e_i$$

- $y_i$ --- наблюдаемое значение зависимой переменной
- $\hat y_i$ --- предсказанное значение зависимой переменной
- $e_i$ --- остатки (отклонения наблюдаемых от предсказанных значений)
]

- $b_0$ --- отрезок (Intercept), отсекаемый регрессионной прямой на оси $y$
- $b_1$ --- коэффициент угла наклона регрессионной прямой

---

class: middle, center, inverse

# Подбор коэффициентов линейной регрессии

---

## Как провести линию регрессии?

$$\hat y _i = b _0 + b _1 x _i$$

```{r echo=FALSE}
teeth_lm <- lm(birth ~ c14, data = teeth)
gg_teeth + 
  geom_abline(slope = coef(teeth_lm)[2], 
              intercept = coef(teeth_lm)[1], 
              colour = 'steelblue3', size = 1) +
  geom_abline(slope = -0.06426, intercept = 1993.76737, 
              colour = 'orange3', size = 1) +
  geom_abline(slope = -0.04226, intercept = 1990.26737,
              colour = 'green4', size = 1)
```

Нужно получить оценки $b_0$ и $b_1$ значений параметров линейной модели $\beta _0$ и $\beta _1$.

Но как это сделать?

---

## Метод наименьших квадратов --- один из способов подбора параметров

$$\hat y _i = b _0 + b _1 x _i$$

Оценки параметров линейной регрессии $b_0$ и $b_1$ подбирают так, чтобы минимизировать сумму квадратов остатков  $\sum{\varepsilon^2_i}$, т.е. $\sum{(y _i - \hat y _i)^2}$.


```{r echo=FALSE}
gg_teeth + 
  geom_abline(slope = coef(teeth_lm)[2], 
              intercept = coef(teeth_lm)[1], 
              colour = 'steelblue3', size = 1) +
  geom_segment(aes(xend =c14 , yend = predict(teeth_lm)), linetype = 'dashed')
```

---

## Памятка с необходимыми для понимания всего происходящего величинами

.small[
| Величина | Что она такое |
| -------- | --------------- |
| $y_i$ | наблюдаемое значение зависимой переменной |
| $\hat y_i$ | предсказанное значение зависимой переменной |
| $\bar y$ | среднее значение зависимой переменной |
| $\bar x$ | среднее значение предиктора |
| $e_i$ | остатки (отклонения наблюдаемых от предсказанных значений) |
| $b_0$ | отрезок (Intercept), отсекаемый регрессионной прямой на оси $y$
| $b_1$ | коэффициент угла наклона (slope) регрессионной прямой |
| $SS$| сумма квадратов |
| $MS$ | стандартизованное значение суммы квадратов|
| $SE$ | стандартная ошибка --- отклонение значения | 
| $df$ | количество степеней свободы |
| $_ t$ | что-то общее для модели (от _total_) |
| $_ r$ | объяснённая часть модели (от _regression_) |
| $_ e$| случайная часть модели (от _error_) |
]

---

## Оценки параметров линейной регрессии

| Параметр | Оценка | Стандартная ошибка |
| -------- | ------ | ------------------ |
| $\beta_0$ | $b_0 = \bar y - b_1 \bar{x}$ | $SE _{b _0} = \sqrt{MS _e [\cfrac{1}{n} + \cfrac{\bar x}{\sum {(x _i - \bar x)^2}}]}$ |
| $\beta_1$ | $b _1 = \cfrac {\sum {[(x _i - \bar {x})(y _i - \bar {y})]}}{\sum {(x _i - \bar x)^2}}$ | $SE _{b _1} = \sqrt{\cfrac{MS _e}{\sum {(x _i - \bar {x})^2}}}$ |
| $\varepsilon _i$ | $e_i = y_i - \hat {y}_i$ | $\approx \sqrt{MS_e}$ |


.tiny[Таблица из кн. Quinn, Keough, 2002, стр. 86, табл. 5.2]

Стандартные ошибки коэффициентов

  - используются для построения доверительных интервалов;
  - нужны для статистических тестов.

---

## Неопределенность оценки положения регрессии

__Доверительный интервал коэффициента__ --- это зона, в которой при повторных выборках из генеральной совокупности с заданной вероятностью будет лежать среднее значение оценки коэффициента. Если $\alpha = 0.05$, то получается 95% доверительный интервал.


$$b _1 \pm t _{\alpha, df = n - 2} \cdot SE _{b _1}$$

__Доверительная зона регрессии__ --- это зона, в которой при повторных выборках из генеральной совокупности с заданной вероятностью лежит регрессионная прямая. 

```{r, teeth-conf, echo=FALSE, purl=FALSE}
gg_teeth + geom_smooth(method = 'lm')
```
---

## Зависимость в генеральной совокупности 

.pull-left[
Симулированный пример: Генеральная совокупность, в которой связь между Y и X, описывается следующей зависимостью
$$
y_i = 10 + 10x_i + \varepsilon_i \\
\varepsilon \in N(0, 20)
$$

```{r pop-code, fig.show='hide'}
pop_x <- rnorm(1000, 10, 3)
pop_y <- 10 + 10*pop_x + rnorm(1000, 0, 20)
population <- data.frame(x = pop_x, y = pop_y)

pop_plot <- ggplot(population, aes(x = x, y = y)) + 
  geom_point(alpha = 0.3, color = "red") + 
  geom_abline(aes(intercept = 10, slope = 10), 
              color="blue", size = 2) +
  theme(text = element_text(size = 15))
pop_plot
```
]

.pull-right[
```{r pop-plot, echo = FALSE, fig.height = 5}
pop_plot 
```
]

---

## Зависимости, выявленные в нескольких разных выборках 

.pull-left[
Линии регрессии, полученные для 100 выборок (по 20 объектов в каждой), взятых из одной и той же генеральной совокупности.
]

.pull-right[

```{r, echo=FALSE, fig.align='center', fig.width=6, fig.height = 6}
samp_coef <- data.frame(b0 = rep(NA, 100), b1 = rep(NA, 100))
for(i in 1:100) {
  samp_num <- sample(1:1000, 20)
  samp <- population[samp_num, ]
  fit <- lm(y ~ x, data = samp)
  samp_coef$b0[i] <- coef(fit)[1]
  samp_coef$b1[i] <- coef(fit)[2]
  
 }

ggplot(population, aes(x = x, y = y)) + 
  geom_point(alpha = 0.3, color = "red") +
  geom_abline(aes(intercept = b0, slope = b1), data = samp_coef) +
  geom_abline(aes(intercept = 10, slope = 10), color = "blue", size = 2) +
  theme(text = element_text(size = 18))
```
]

---

## Неопределенность оценок предсказанных значений

__Доверительный интервал к предсказанному значению__ --- это зона, в которую попадает заданная доля значений $\hat y _i$ при данном $x _i$.


$\hat y _i \pm t _{\alpha, n - 2} \cdot SE _{\hat y _i}$, $SE _{\hat y} = \sqrt{MS _{e} [1 + \frac{1}{n} + \frac{(x _{prediction} - \bar x)^2} {\sum _{i=1}^{n} {(x _{i} - \bar x)^2}}]}$

__Доверительная область значений регрессии__ --- это зона, в которую попадает $(1 - \alpha) \cdot 100\%$ всех предсказанных значений.

```{r, teeth-pr-all, echo=FALSE, results='hide', purl=FALSE}
pr_all <- predict(teeth_lm, interval = 'prediction')
teeth_with_pred <- data.frame(teeth, pr_all)
head(teeth_with_pred)
```

```{r, teeth-pred, echo=FALSE, purl=FALSE}
gg_teeth + geom_smooth(method = 'lm', se = FALSE) +
  geom_ribbon(data = teeth_with_pred, 
              aes(y = fit, ymin = lwr, ymax = upr), 
              fill = 'green', alpha = 0.2)
```

<!-- --- -->

<!-- ## Стандартизованные коэффициенты -->

<!-- Стандартизованные коэффициенты используются для сравнения вкладов предикторов в изменение отклика. Их можно использовать при сравнении разных моделей. -->

<!-- - Не зависят от масштаба измерений x и y -->
<!-- - Можно вычислить, зная обычные коэффициенты и их стандартные отклонения $b^\ast _1 = {b _1  \frac {\sigma _x} {\sigma _y}}$ -->
<!-- - Можно вычислить, посчитав регрессию по стандартизованным данным -->

---

class: middle, center, inverse

# Линейная регрессия в R

---

## Как в R задать формулу линейной регрессии

`lm(formula = формула_модели, data = данные)` - функция для подбора регрессионных моделей.

Формат формулы модели: `зависимая_переменная ~ независимые_переменные`.

$\hat y _i = b _0 + b _1 x _i$ (простая линейная регрессия с $b _0$ (intercept))

- Y ~ X
- Y ~ 1 + X 
- Y ~ X + 1

$\hat y _i = b _1 x _i$ (простая линейная регрессия без $b _0$)

- Y ~ X - 1
- Y ~ -1 + X

$\hat y _i = b _0$ (уменьшенная модель, линейная регрессия Y от $b _0$)

- Y ~ 1
- Y ~ 1 - X

---

## Другие примеры формул линейной регрессии

$\hat y _i = b _0 + b _1 x _{1 i} + b _2 x _{2 i} + b _3 x _{3 i}$

(множественная линейная регрессия с $b _0$)

- Y ~ X1 + X2 + X3
- Y ~ 1 + X1 + X2 + X3

$\hat y _i = b _0 + b _1 x _{1 i} + b _3 x _{3 i}$

(уменьшенная модель множественной линейной регрессии, без $x _2$)

- Y ~ X1 + X3
- Y ~ 1 + X1 + X3

---

## Задание 2

Используя данные из датасета `teeth`, подберите модель линейной регрессии, описывающую зависимость рода рождения человека `birth` от количества изотопа $^{14}C$ `c14`. 

Запишите коэффициенты модели и уравнение линейной регрессии.

Подсказки:

`lm(formula = формула_модели, data = данные)` --- функция для подбора регрессионных моделей

Формат формулы модели: `зависимая_переменная ~ независимые_переменные`

`summary(модель)` --- функция, показывающая краткую информацию о модели в виде таблицы

`coef(модель)` --- функция, показывающая только коэффициенты модели


```
teeth_lm <- lm(formula = , data = )
```

---

## Решение: Подбираем параметры линейной модели


```{r, teeth-reg, purl=FALSE}
teeth_lm <- lm(formula = birth ~ c14, data = teeth)
summary(teeth_lm)
```

Коэффициенты линейной регрессии:

- $b _0 =  `r format(coef(teeth_lm)[1], digits = 2)` \pm `r format(coef(summary(teeth_lm))[1, 2], digits = 1)`$
- $b _1 =  `r format(coef(teeth_lm)[2], digits = 2)` \pm `r format(coef(summary(teeth_lm))[2, 2], digits = 1)`$

---

## Решение: Записываем уравнение линейной регрессии

Модель:

$$\hat y _i = b _0 + b _1 x _i$$

Коэффициенты:

```{r, purl=FALSE}
coef(teeth_lm)
```

Уравнение регрессии:  

$$\widehat{birth} _i  = 0.2 + 5.4 * c14 _i$$
---

class: middle, center, inverse

# Тестирование значимости модели и ее коэффициентов

---

## Способы проверки значимости модели и ее коэффициентов

Существует несколько способов проверки значимости модели

Значима ли модель целиком?

+ F-критерий: действительно ли объясненная моделью изменчивость больше, чем случайная ( = остаточная) изменчивость.

Значима ли связь между предиктором и откликом?

+ t-критерий: отличается ли от нуля коэффициент при этом предикторе;
+ F-критерий: действительно ли объясненная предиктором изменчивость больше, чем случайная ( = остаточная)?

---

## Тестируем значимость коэффициентов t-критерием

$$t = \frac{b _1}{SE _{b _1}}$$

$H _0 : b _1 = 0$.
$H _A : b _1 \ne 0$

$t$-статистика подчиняется $t$-распределению с числом степеней свободы $df = n - p$, где $p$ --- число параметров.

Для простой линейной регрессии $df = n - 2$.

---

## Тестируем значимость коэффициентов t-критерием

```{r}
summary(teeth_lm)
```

Результаты можно описать в тексте так:

- Дата рождения значимо возрастает с уменьшением содержания изотопа $^{14}C$ $(b _1 =$ `r  round(coef(teeth_lm)[2], 2)`, $t_{} =$ `r round(summary(teeth_lm)$coefficients[2, 3], 2)`, $p < 0.01$)

---

## Тестируем значимость модели целиком при помощи F-критерия

$$F =\frac{MS _{regression}}{MS _{error}}$$

$H _0: \beta _1 = 0$ 

Число степеней свободы $df _{regression}$, $df _{error}$


```{r echo=FALSE, purl=FALSE}
lims <- range(teeth$birth) + c(-1, 1)
yannot <- lims[1] + 0.5
xannot <- max(teeth$c14)
gmean <- mean(teeth$birth, na.rm = TRUE)
Y <- 0.84
Y_hat <- predict(teeth_lm, newdata = teeth[teeth$birth == 0.84, ])
X <- teeth$c14[teeth$birth == 0.84]

# Общая изменчивость
pl_tot <- ggplot(teeth, aes(x = c14, y = birth)) + 
  geom_hline(yintercept = gmean, size = 1) + 
  geom_segment(aes(x = c14, y = birth, 
                   xend = c14, yend = gmean), colour = "grey70") + 
  geom_point() +
  annotate("text", label = "Общее\nсреднее", 
           x = max(teeth$c14), y = gmean, size = 4, hjust = 1, vjust = 1.3) + 
  labs(x = 'Содержание нестабильного изотопа углерода', y = 'Год рождения') +
  ggtitle("Общая изменчивость")
  # annotate("text", label = "SS[t] == sum((bar(y) - y[i]))^2", parse = TRUE, x = xannot,  y = yannot, hjust = 0.95, vjust = 0.2, size = 6)

# Объясненная изменчивость
pl_exp <- ggplot(teeth, aes(x = c14, y = birth)) + 
  geom_smooth(method = "lm", se = F, size = 1.3) + 
  geom_hline(yintercept = gmean, size = 1) + 
  geom_segment(aes(x = c14, y = gmean, 
                   xend = c14, yend = fitted(teeth_lm)), colour = "#E69F00") + 
  geom_point() +
  annotate("text", label = "Общее\nсреднее", 
           x = max(teeth$c14), y = gmean, size = 4, hjust = 1, vjust = 1.3) + 
  labs(x = 'Содержание нестабильного изотопа углерода', y = 'Год рождения') +
  ggtitle("Объясненная") +
      # annotate("text", label = "SS[r] == sum((bar(y) - hat(y)[i]))^2", parse = TRUE, x = xannot,  y = yannot, hjust = 0.95, vjust = 0.2, size = 6)  +
  theme(axis.title.y = element_blank())

# Остаточная изменчивость
pl_res <- ggplot(teeth, aes(x = c14, y = birth)) + 
  geom_smooth(method ="lm", se = F, size = 1.3) + 
  geom_segment(aes(x = c14, y = birth, 
                   xend = c14, yend = fitted(teeth_lm)), colour = "#009E73") + 
  geom_point() +
  labs(x = 'Содержание нестабильного изотопа углерода', y = 'Год рождения') +
  ggtitle("Случайная") +
  # annotate("text", label = "SS[e] == sum(sum((y [i] - hat(y)[i])))^2", parse = TRUE, x = xannot,  y = yannot, hjust = 0.95, vjust = 0.2, size = 6)
  theme(axis.title.y = element_blank())
```

---

## Общая изменчивость

Общая изменчивость --- $SS _{total}$, сумма квадратов отклонений от общего среднего значения.


```{r echo=FALSE, purl=FALSE}
pl_tot
```

---

## Общая изменчивость делится на объясненную и остаточную


.right[
.pull-left-33[
$SS_t = SS_r + SS_e$]
]  

.left[
.pull-right-33[
$MS_t \ne MS_r + MS_e$]
]


```{r arranged_variab, echo=FALSE, purl=FALSE, fig.width=12}
library(gridExtra)
pl_total <- pl_tot + ggtitle('Общая')
grid.arrange(pl_total, pl_exp, pl_res, nrow = 1)
```

![:col_row $SS_{t} = \sum{(y_i - \bar{y})^2}$, $SS_{r} =\sum{(\hat{y}-\bar{y})^2}$,  $SS_{e} = \sum{(y_i - \hat{y})^2}$]
![:col_row $df_{t} = n - 1$, $df_{r} = p - 1$, $df_{e} = n - p$]
![:col_row $MS_{e} = \frac{SS_{e}}{df_{e}}$, $MS_{r} = \frac {SS_{r}}{df_{r}}$, $MS_{t} = \frac {SS_{t}}{df_{t}}$]


<!-- Они не зависят от числа наблюдений в выборке, в отличие от $SSr$ и $SS_e$. -->
<!-- С их помощью можно проверить гипотезу о наличии связи между предиктором и откликом. -->


---

## Если зависимости нет, то коэффициент $b _1 = 0$

Тогда $\hat y _i = \bar y _i$ и $MS _{regression} \approx MS _{error}$. 

Это можно использовать при тестировании гипотезы $\beta _1 = 0$.

```{r arranged_variab, echo=FALSE, purl=FALSE, fig.width=12}
```

![:col_row $SS_{t}= \sum{(y_i - \bar{y})^2}$, $df_{t} = n - 1$,
$MS_{t} = \frac {SS_{t}}{df_{t}}$]
![:col_row $SS_{r}=\sum{(\hat{y}-\bar{y})^2}$, $df_{r} = p - 1$, $MS_{r} = \frac {SS_{r}}{df_{r}}$]
![:col_row $SS_{e}= \sum{(y_i - \hat{y})^2}$, $df_{e} = n - p$, $MS_{e} = \frac{SS_{e}}{df_{e}}$]

---

## F-критерий и распределение F-статистики

Если $b _1 = 0$, тогда $\hat y_i = \bar y_i$ и $MS _{r} \approx MS _{e}$.

F --- соотношение объясненной и не объясненной изменчивости:
$$F = \frac{MS_{regression}}{MS_{error}}$$

Подчиняется F-распределению с параметрами $df _{r}$ и $df _{e}$.

Для простой линейной регрессии $df_{r} = 1$ и $df_{e} = n - 2$.

```{r, echo=FALSE, purl=FALSE}
ar <- arrow(type = "closed", length = unit(0.15,"cm"))
arb <- arrow(type = "closed", length = unit(0.15,"cm"), ends = "both")

dfr <- data.frame(f = seq(-0.001, 2, 0.0001))
ggplot(dfr, aes(x = f)) + 
  stat_function(fun = df, args = list(df1 = 1, df2 = 21), size = 1.3) + 
labs(title = expression(bold(paste("F-распределение,", ~df[1]==1, ", ", ~ df[2]==21))),
     x = "F", y = "Плотность вероятности")
```

---

## Таблица результатов дисперсионного анализа


| Источник <br> изменчивости  | df | SS | MS | F | P |
|---------------------------- | -- | -- | -- | - | - |   
| Регрессия | $df _r = 1$ | $SS_r = \sum{(\hat y_i - \bar y)^2}$ | $MS _r = \frac{SS_r}{df_r}$ | $F_{df_r, df_e} = \frac{MS_r}{MS_e}$ | $p$ |
| Остаточная | $df_e = n - 2$ | $SS_e = \sum{(y _i - \hat y _i)^2}$ | $MS _e = \frac{SS_e}{df_e}$ | |
| Общая | $df_t = n - 1$ | $SS_t = \sum {(y_i - \bar y)^2}$ | | | |

.large[
Минимальное упоминание результатов в тексте должно содержать $F _{df _r, df _e}$ и $p$.
]

---

## Проверяем значимость модели при помощи F-критерия

```{r}
library(car)
Anova(teeth_lm)
```

Результаты дисперсионного анализа можно описать в тексте (или представить в виде таблицы):

```{r echo=FALSE, purl=FALSE}
smr_f <- summary(teeth_lm)$fstatistic
f_val <- round(smr_f[1], 2)
df1 <- smr_f[2]
df2 <- smr_f[3]
```

- Год рождения у трупов значимо зависит от количества радиоактивного углерода в эмали зубов $(F _{`r df1`, `r df2`} = `r f_val`$, $p < 0.001)$.

---

class: middle, center, inverse

# График линейной регрессии

---

## Задание 3 

Дополните график `gg_teeth`, чтобы построить: 
- 95% доверительную зону регрессии,
- 99% доверительную зону регрессии.

Используйте `geom_smooth()` и его аргументы `method` и `level`

```{r eval=FALSE}
gg1 <- gg_teeth + 
  labs(title = '95% доверительная зона')
gg1
gg2 <- gg_teeth + 
  labs(title = '99% доверительная зона')
gg2

library(cowplot)
plot_grid(gg1, gg2, nrow = 1, labels = 'AUTO')
```

---

## Решение: Строим доверительную зону регрессии

```{r conf-int-regr, purl=FALSE, out.width='100%', fig.height = 6, fig.width = 12}
gg1 <- gg_teeth + geom_smooth(method = 'lm') + 
  labs(title = '95% доверительная зона')
gg2 <- gg_teeth + geom_smooth(method = 'lm', level = 0.99) + 
  labs(title = '99% доверительная зона')
library(cowplot)
plot_grid(gg1, gg2, nrow = 1, labels = 'AUTO')
```

---

class: middle, center, inverse

# Оценка качества подгонки модели

---

## Коэффициент детерминации $R^2$

Доля общей изменчивости, объясненная линейной связью x и y

$$R^2 =\frac{SS_{r}}{SS_{t}} = 1 - \frac{SS_{e}}{SS_{t}}$$

$$0 \le R^2 \le 1$$

Иначе рассчитывается как квадрат коэффициента корреляции $R^2 = r^2$

__Не используйте $R^2$ для множественной линейной регрессии!!!__

---

## Коэффициент детерминации можно найти в сводке модели

```{r}
summary(teeth_lm)
```

---

## Сравнение качества подгонки моделей при помощи $R^2_{adj}$

$R^2_{adj}$ --- cкорректированный $R^2$

$$R^2_{adj} = 1 - \frac{SS_{e} / df_{e}}{SS_{t} / df_{t}}$$

где $df_{e} = n - p$, $df_{t} = n - 1$

$R^2_{adj}$ учитывает число переменных в модели, вводится штраф за каждый новый параметр.

Используйте $R^2 _{adj}$ для сравнения моделей с разным числом параметров.

---

class: middle, center, inverse

# Использование линейной регрессии для предсказаний

---

## Использование линейной регрессии для предсказаний

Для конкретного значения предиктора мы можем сделать два типа предсказаний:

- предсказываем среднее значение отклика --- это оценка точности положения линии регрессии;
- предсказываем значение отклика у 95% наблюдений --- это оценка точности предсказаний.

---

## Предсказываем Y при заданном X 

В каком году родились люди с количеством $^{14}C$ равным 125, 314,  и 565?

Значения, для которых предсказываем:

```{r}
new_data1 <- data.frame(c14 = c(125, 314, 565)) 
new_data1
```

---

## Предсказываем Y при заданном X 

Предсказания:

```{r}
pr1 <- predict(teeth_lm, newdata = new_data1, 
                interval = 'confidence', se = TRUE)
pr1$fit
```

- Люди с содержанием $^{14}C$ 125, 314 и 565% родились в `r round(pr1$fit[1,1], 1)` $\pm$ `r round(pr1$fit[1,1] - pr1$fit[1,2], 1)`, `r round(pr1$fit[2,1], 1)` $\pm$ `r round(pr1$fit[2,1] - pr1$fit[2,2], 1)` и  `r round(pr1$fit[3,1], 1)` $\pm$ `r round(pr1$fit[3,1] - pr1$fit[3,2], 1)` годах, соответственно.

---

## Предсказываем изменение Y для 95% наблюдений при заданном X

В пределах каких годов родились люди, у которых содержание $^{14}C$ в эмали соответствует 125, 314  и 565 %?

```{r}
# значения, для которых предсказываем
(pr2 <- predict(teeth_lm, newdata = new_data1, 
                interval = 'prediction', se = TRUE))
```

- 95% умерших людей, у которых содержание радиоактивного углерода равно 125, 314  и 565 %, родились в пределах `r round(pr2$fit[1,1], 1)` $\pm$ `r round(pr2$fit[1,1] - pr2$fit[1,2], 1)`, `r round(pr2$fit[2,1], 1)` $\pm$ `r round(pr2$fit[2,1] - pr2$fit[2,2], 1)` и `r round(pr2$fit[3,1], 1)` $\pm$ `r round(pr2$fit[3,1] - pr2$fit[3,2], 1)` соответственно.

---

## Построим график доверительной области значений

Создадим данные для графика. 

Для этого объединим в новом датафрейме:

- исходные данные;
- предсказанные значения для исходных данных. 

```{r, teeth-pr-all}
```


---

## Строим доверительную область значений и доверительный интервал одновременно

```{r, teeth-plot-all}
gg_teeth + 
  geom_smooth(method = 'lm', 
              aes(fill = 'Доверительный \nинтервал'), 
              alpha = 0.4) +
  geom_ribbon(data = teeth_with_pred, 
              aes(y = fit, ymin = lwr, ymax = upr, 
                  fill = 'Доверительная \nобласть значений'), 
              alpha = 0.2) +
  scale_fill_manual('Интервалы', values = c('green', 'blue'))
```

---

## Take home messages

- Модель простой линейной регрессии $y _i = \beta _0 + \beta _1 x _i + \varepsilon _i$
- В оценке коэффициентов регрессии $(b_0$ и $b_1)$ и предсказанных значений $(\hat y_i)$ существует неопределенность. Доверительные интервалы можно рассчитать, зная стандартные ошибки.
- Значимость всей регрессии и ее параметров можно проверить при помощи t- или F-теста. Для простой линейной регрессии $H _0: \beta _1 = 0$.
- Качество подгонки модели можно оценить при помощи коэффициента детерминации $R^2$
- Не всякие данные можно описать при помощи простой линейной регрессии.

---

## Дополнительные ресурсы

- Гланц, 1999, стр. 221-244
- OpenIntro: Statistics
- Quinn, Keough, 2002, pp. 78-110
- Logan, 2010, pp. 170-207
- Sokal, Rohlf, 1995, pp. 451-491
- Zar, 1999, pp. 328-355
- Видосики на ютубе от [StatQuest](https://www.youtube.com/@statquest) 

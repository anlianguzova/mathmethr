---
title: "Регрессионный анализ, часть 2"
subtitle: "Математические модели в зоологии"
author: 
  - Марина Варфоломеева
  - Анастасия Лянгузова
company: 'Каф. Зоологии беспозвоночных, СПбГУ'
output:
  xaringan::moon_reader:
    self-contained: true
    lib_dir: libs
    css: [ninjutsu, "assets/xaringan-themer.css", "assets/xaringan.css"]
    df_print: default
    nature:
      highlightStyle: vs
      highlightLines: true
      countIncrementalSlides: false
      titleSlideClass: [middle, left, inverse]
      beforeInit: "assets/cols_macro.js"
    includes:
      in_header: "assets/xaringan_in_header.html"
      after_body: "assets/xaringan_after_body.html"
---

```{r setup, include = FALSE, cache = FALSE, purl = TRUE}
source("assets/xaringan_setup.R")
library(xaringanExtra)
use_tile_view()
use_scribble()
use_search(show_icon = FALSE)
use_progress_bar(color = "#98BF64", location = "bottom", height = "10px")
use_freezeframe()
# use_webcam()
# use_panelset()
# use_extra_styles(hover_code_line = TRUE)

# http://tachyons.io/docs/
# https://roperzh.github.io/tachyons-cheatsheet/
use_tachyons()
source('support_mathmethr.R')
```

### Вы сможете

- Подобрать модель множественной линейной регрессии
- Протестировать значимость модели и ее коэффициентов
- Интерпретировать коэффициенты множественной регрессии при разных предикторах
- Проверить условия применимости простой и множественной линейной регрессии при помощи анализа остатков


---

class: middle, center, inverse

# Множественная линейная регрессия 

---

## Пример: реки штата Нью-Йорк

В 70-е годы в штате Нью Йорк обследовали 20 речных бассейнов (Haith, 1976), чтобы оценить качество воды. Как влияют особенности землепользования на среднюю концентрацию азота (мг/л) в воде?  
(Датасет river из пакета bstats, источник Chatterjee & Hadi, 2006)

20 рек в штате Нью Йорк

- `River`	- название реки
- `Agr` - процент сельскохозяйственных земель
- `Forest` - процент земли, занятой лесом
- `Rsdntial` - процент земель, занятых поселениями
- `ComIndl`	- процент земель, занятых коммерцией и промышленностью
- `Nitrogen` - средняя концентрация азота в воде, мг/л

Т.е. мы хотим подобрать модель вида:

$Nitrogen_i = b_0 + b_1 Agr_i + b_2 Forest_i + b_3 Rsdntial_i + b_4 ComIndl_i + e_i$

---

## Читаем данные из файла одним из способов

### Чтение из xlsx
```{r}
library(readxl)
river <- read_excel(path = "data/river.xlsx", sheet = "river-data")
```

### Чтение из csv

```{r}
river <- read.table("data/river.csv", header = TRUE, sep = "\t")
```

---

## Все ли правильно открылось?

```{r}
str(river)      # Структура данных
head(river)     # Первые несколько строк файла
```

---

## Знакомимся с данными

Есть ли пропущенные значения?

```{r}
colSums(is.na(river))
```

Каков объем выборки?

```{r}
nrow(river)
```

---

## Парные графики для всех числовых переменных

```{r eval=FALSE}
pairs(river[, -1])
```

```{r pairs, fig.width = 9, fig.height=4.4, echo=FALSE, purl=FALSE}
pairs(river[, -1], gap = 0.25, oma=rep(1.75, 4))
```

--

__Выброс__ — сильно отскакивающее значение. 

Похоже, что в этих данных есть выброс в столбце `Rsdntial`.

---

## Варианты действий с выбросами

- удалить это наблюдение, т.к. рек с таким большим уровнем застройки территории больше нет в датасете.

- трансформировать `Rsdntial` (извлечь логарифм), чтобы "растянуть" начало шкалы и "сплющить" ее конец.

Мы пока продолжим, чтобы посмотреть как будет выглядеть это значение при проверке условий применимости регрессии (об этом чуть дальше).

---

## Задача

1. Подберите модель множественной линейной регрессии, чтобы описать, как зависит концентрация азота от особенностей землепользования.

$Nitrogen_i = b_0 + b_1 Agr_i + b_2 Forest_i + b_3 Rsdntial_i + b_4 ComIndl_i + e_i$

1. Запишите уравнение этой линейной модели с коэффициентами.

---

## Решение

```{r purl=FALSE}
river_lm1 <- lm(Nitrogen ~ Agr + Forest + Rsdntial + ComIndl, data = river)
# summary(river_lm1)
```

Коэффициенты модели:

```{r purl=FALSE}
coef(river_lm1)
```

Уравнение регрессии:

$`r lm_equation(river_lm1, strict=FALSE, digits = 1)`$

--

Более формальная запись  
(и та и другая запись требует расшифровки обозначений):  

$`r lm_equation(river_lm1, digits = 1)`$

--

__Важно!__ Прежде чем интерпретировать результаты нужно обязательно проверить, выполняются ли условия применимости линейной регрессии.

---

class: middle, center, inverse

# Условия применимости линейной регрессии

---

## Условия применимости линейной регрессии 

Условия применимости линейной регрессии должны выполняться, чтобы тестировать гипотезы

1. Независимость
1. Линейность 
1. Нормальное распределение
1. Гомогенность дисперсий
1. Отсутствие коллинеарности предикторов (для множественной регрессии с этого условия нужно начинать!)

---

## 1. Независимость

- Значения $y _i$ должны быть независимы друг от друга
- Берегитесь псевдоповторностей и автокорреляций (например, временных)
- Контролируется на этапе планирования
- Проверяем на графике остатков

.center[
![](images/assumption-12.png)

.tiny[
Из кн. Diez et al., 2010, стр. 332, рис. 7.8
]
]

---

## 2. Линейность связи

- Проверяем на графике рассеяния исходных данных
- Проверяем на графике остатков

.center[
![](images/assumption-12.png)

.tiny[
Из кн. Diez et al., 2010, стр. 332, рис. 7.8
]
]

---

## Что бывает, если не глядя применять линейную регрессию

.pull-left[
[Квартет Энскомба](http://ru.wikipedia.org/wiki/Квартет_Энскомба) - примеры данных, где регрессии одинаковы во всех случаях (Anscombe, 1973)

$y _i = 3.0 + 0.5 x _i$

$r^2 = 0.68$

$H _0: \beta _1 = 0, t = 4.24, p = 0.002$
]

.pull-right[
![](images/anscombe.png)

.tiny[
Из кн. Quinn, Keough, 2002, стр. 97, рис. 5.9
] 
]

---

## 3. Нормальное распределение остатков

.pull-left[
Нужно, т.к. в модели $Y _i = \beta _0 + \beta x _i + \epsilon _i$ зависимая переменная $Y \sim N(0,\sigma^2)$, а значит $\epsilon _i \sim N(0,\sigma^2)$

- Нужно для тестов параметров, а не для подбора методом наименьших квадратов
- Нарушение не страшно --- тесты устойчивы к небольшим отклонениям от нормального распределения
- Проверяем распределение остатков на нормально-вероятностном графике (normal QQ-plot)
]

.pull-right[
.pull-left[
```{r qqplot-norm-res, purl=FALSE, echo=FALSE, fig.height=8}
set.seed(27828)
library(car)
library(ggplot2)

x_norm <- rnorm(50, mean = 10, sd = 1)
y_norm <- rnorm(50, 3, 3) 
data_norm <- data.frame(x_norm, y_norm)
model_norm <- lm(y_norm ~ x_norm, data_norm)
res_norm <- residuals(model_norm)
qqnorm(res_norm)

x_exp <- x_norm
y_exp <- 3 + 0.8 * x_exp ^ 4
data_exp <- data.frame(x_exp, y_exp)
model_exp <- lm(y_exp ~ x_exp, data_exp)
res_exp <- residuals(model_exp)
qqnorm(res_exp)
```
]

.pull-right[
```{r qqplot-log-res, echo=FALSE, message=FALSE, warning=FALSE, purl=FALSE, fig.height=8}
library(lme4)
x_log <- x_norm
y_log <- 3 + log(x_log)
data_log <- data.frame(x_log, y_log)
model_log <- lm(y_log ~ x_log, data_log)
res_log <- residuals(model_log)
qqnorm(res_log)

x_discrete <- x_norm
y_discrete <- rep(c(1, 2, 3, 4, 5), 10) 
data_discrete <- data.frame(x_discrete, x_norm, y_discrete)
model_discrete <- glmer(y_discrete ~ x_discrete + (1 | x_norm) , data_discrete, 'poisson')
res_discrete <- residuals(model_discrete)
qqnorm(res_discrete)
```
]
]

---
  
## 4. Гомогенность дисперсий

.pull-left[
Нужно, т.к. в модели $Y _i = \beta _0 + \beta x _i + \epsilon _i$ зависимая переменная $Y \sim N(0,\sigma^2)$ и дисперсии $\sigma^2 _1 = \sigma^2 _2 = ... = \sigma^2 _i$ для каждого $Y _i$ 

Поскольку $\epsilon _i \sim N(0,\sigma^2)$, можно проверить равенство дисперсий остатков $\epsilon _i$

- Нужно и важно для тестов параметров
- Проверяем на графике остатков по отношению к предсказанным значениям
- Есть формальные тесты, но они очень чувствительны (тест Бройша-Пагана, тест Кокрана)
]

.pull-right[
```{r gg-norm-tunnel, echo=FALSE, purl=FALSE}
## Based on code by Arthur Charpentier:
## http://freakonometrics.hypotheses.org/9593
## TODO: wrap it into a function and adapt it for use with other distributions
## as Markus Gesmann has done here
## http://www.magesblog.com/2015/08/visualising-theoretical-distributions.html

n <- 2
brain <- read.csv("data/IQ_brain.csv", header = TRUE)
X <- brain$MRINACount 
Y <- brain$PIQ
dfr <- data.frame(X, Y)

# regression
reggig <- glm(Y ~ X, data = dfr, family = gaussian(link = "identity"))

op <- par(mar = c(0, 0, 0, 0))
# empty plot
vX <- seq(min(X) - 0.5, max(X) + 0.5, length = n)
vY <- seq(min(Y) - 50, max(Y) + 50, length = n)
mat <- persp(x = vX, y = vY, z = matrix(0, n, n), 
             zlim = c(0, 0.0001),
             theta =  - 30, phi = 20, expand = 0.0018,
             ticktype  = "detailed",  box = FALSE, border = "gray60")

x <- seq(min(X), max(X), length = 501)

# expected values
C <- trans3d(x, predict(reggig, newdata = data.frame(X = x), type = "response"), rep(0, length(x)), mat)
lines(C, lwd = 2)

sdgig <- sqrt(summary(reggig)$dispersion)

# 1SD
y1 <- qnorm(.95, predict(reggig, newdata = data.frame(X = x), type = "response"),  sdgig)
C <- trans3d(x, y1, rep(0, length(x)), mat)
lines(C, lty = 2, col = "#d95f02")
y2 <- qnorm(.05, predict(reggig, newdata = data.frame(X = x), type = "response"),  sdgig)
C <- trans3d(x, y2, rep(0, length(x)), mat)
lines(C, lty = 2, col = "#d95f02")

# C <- trans3d(c(x, rev(x)), c(y1, rev(y2)), rep(0, 2 * length(x)), mat)
# polygon(C, border = NA, col = "yellow")

# data points
C <- trans3d(X, Y, rep(0, length(X)), mat)
points(C, pch = 1, col = "black", cex = 0.4)

# density curves
n <- 6
vX <- seq(min(X), max(X), length = n)

mgig <- predict(reggig, newdata = data.frame(X = vX))
sdgig <- sqrt(summary(reggig)$dispersion)

for(j in n:1){
  stp <- 251
  x <- rep(vX[j], stp)
  y <- seq(min(min(Y) - 50, 
               qnorm(.05, 
                     predict(reggig, 
                             newdata = data.frame(X = vX[j]), 
                             type = "response"),  
                     sdgig)), 
           max(Y) + 50, 
           length = stp)
  z0 <- rep(0, stp)
  z <- dnorm(y,  mgig[j],  sdgig)
  C <- trans3d(c(x, x), c(y, rev(y)), c(z, z0), mat)
  polygon(C, border = NA, col = "light blue", density = 40)
  C <- trans3d(x, y, z0, mat)
  lines(C, lty = 2, col = "grey60")
  C <- trans3d(x, y, z, mat)
  lines(C, col = "steelblue")
}
par(op)
```
]
---

## Гетероскедастичность

```{r heterosced, echo=FALSE, eval = TRUE, purl=FALSE, fig.width=7}
library(cowplot)
N <- 300
b_0 <- 0.5
b_1 <- 8

set.seed(123456)
x <- rnorm(N, 10, 3)
eps_1 <- rnorm(N, 0, 10)
y_1 <- b_0 + b_1*x + eps_1

# |v|^(2*t), t = 0.7
h <- function(x) x^(2*0.7) 
eps_2 <- rnorm(N, 0, h(x))
y_2 <- b_0 + b_1*x + eps_2
dat <- data.frame(x, y_1, y_2)
dat$log_y <- log(y_2)

pl_hom <- ggplot(dat, aes(x = x, y = y_1)) + geom_point(alpha = 0.5) + geom_smooth(method = "lm", alpha = 0.7) + ggtitle("Гомоскедастичность") + ylab("Y")
pl_heter <- pl_hom + aes(y = y_2) + ggtitle("Гетероскедастичность") + ylab("Y")

dat_diag_1 <- fortify(lm(y_1 ~ x, data = dat))
dat_diag_2 <- fortify(lm(y_2 ~ x, data = dat))

pl_hom_resid <- ggplot(dat_diag_1, aes(x = .fitted, y = .stdresid)) + geom_point(alpha = 0.5) + geom_smooth(se=FALSE, method = "loess")
pl_heter_resid <- pl_hom_resid %+% dat_diag_2

plot_grid(pl_hom, pl_heter, 
          pl_hom_resid, pl_heter_resid, 
          ncol = 2, rel_heights = c(0.55, 0.45))
```

Бороться можно трансформацией $Y$ --- например, логарифмированием. 

---

## Чем опасна гетероскедастичность

Создадим две генеральные совокупности, в которых нет связи между $y$ и $x$
```{r echo = FALSE, fig.width=10}
N <- 1000
b_0 <- 0
b_1 <- 0

set.seed(123456)
x <- rnorm(N, 10, 3)
eps_1 <- rnorm(N, 0, 10)
y_1 <- b_0 + b_1*x + eps_1

h <- function(x) x^(2*0.7) 
eps_2 <- rnorm(N, 0, h(x))
y_2 <- b_0 + b_1*x + eps_2
dat2 <- data.frame(x, y_1, y_2)

pl_hom <- 
  ggplot(dat2, aes(x = x, y = y_1)) + geom_point(alpha = 0.5)  + ggtitle("Гомоскедастичность") + ylab("Y")

pl_heter <- 
  pl_hom + aes(y = y_2) + ggtitle("Гетероскедастичность") + ylab("Y")

plot_grid(pl_hom, pl_heter)
```

---

## Чем опасна гетероскедастичность

.pull-left-60[
Возьмем 1000 выборок из этих совокупностей по 10 наблюдений и построим много линейных моделей

```{r}
p_values <- data.frame(p_no_heter = rep(NA, 1000), p_heter = NA)

for(i in 1:1000){
  df <- dat2[sample(1:1000, 10), ]
  M_no_heter <- lm(y_1 ~ x, data = df)
  M_heter <- lm(y_2 ~ x, data = df)
  p_values$p_no_heter[i] <- summary(M_no_heter)$coefficients[2, 4] #p-value из summary модели 
  p_values$p_heter[i] <- summary(M_heter)$coefficients[2, 4] #p-value из summary модели     
  }
```
]

.pull-right-40[


- Нет гетероскедастичности и нет связи между $y$ и $x$ (справедлива $H_0$) частота ошибок составляет 

```{r}
mean(p_values$p_no_heter < 0.05)

```


- Для совокупности с гетероскедастичностью, при отсутствии связи между  $y$ и $x$ (справедлива $H_0$) частота ошибок составляет 

```{r}
mean(p_values$p_heter < 0.05)

```
]

---

## Диагностика регрессии по графикам остатков

.pull-left[
![](images/assumption-violations-on-residual-plots.png)

.tiny[
Из кн. Logan, 2010, стр. 174, рис. 8.5 d] 
]

--

.pull-right[
- (a)все условия выполнены  
- (b)разброс остатков разный (wedge-shaped pattern)  
- (c)разброс остатков одинаковый, но нужны дополнительные предикторы  
- (d)к нелинейной зависимости применили линейную регрессию  
]

---

## Задача: Проанализируйте графики остатков

- какой регрессии соответствует какой график остатков?
- все ли условия применимости регрессии здесь выполняются?
- назовите случаи, в которых можно и нельзя применить линейную регрессию?

.pull-left[
![](images/assumption-quiz1.png)
]

.pull-right[
![](images/assumption-quiz2.png)
]

.tiny[Из кн. Watkins et al. 2008, стр. 177, рис. 3.84-3.85]

---

## Решение

- A-I - нелинейная связь - нельзя; 
- B-II - все в порядке, можно; 
- C-III - все в порядке, можно; 
- D-IV - синусоидальный паттерн в остатках, нарушено условие независимости или зависимость нелинейная - нельзя.

.pull-left[
![](images/assumption-quiz1.png)
]

.pull-right[
![](images/assumption-quiz2.png)
]


.tiny[Рис. из кн. Watkins et al. 2008, стр. 177, рис. 3.84-3.85]

---

## Коллинеарность предикторов

### Коллинеарность

> Коллинеарные предикторы коррелируют друг с другом, т.е. не являются взаимно независимыми


Последствия:

- Модель неустойчива к изменению данных;
- При добавлении или исключении наблюдений может меняться оценка и знак коэффициентов.

Что делать с коллинеарностью?

- Удалить из модели избыточные предикторы;
- Получить вместо скоррелированных предикторов один новый; комбинированный при помощи метода главных компонент.

---

## Проверка на коллинеарность

### Показатель инфляции для дисперсии

(коэффициент распространения дисперсии, Variance inflation factor, VIF)

$VIF$ оценивает степень избыточности каждого из предикторов модели:

$$VIF = 1/(1-R'^2)$$

__Здесь в знаменателе используется $R^2$ регрессии данного предиктора от всех других предикторов в модели__.

Хорошо, если $VIF < 10$ (по Marquardt, 1970), но лучше $VIF < 3$, а иногда и $VIF < 2$. Если больше --- есть коллинеарность.

Предикторы с $VIF$ больше порогового значения нужно последовательно удалить из модели (по-одному, проверяя, как изменился $VIF$ после каждого этапа удаления).

---

## Влиятельные наблюдения

Ненадолго вернёмся к нашим данным по рекам. 

```{r pairs, fig.width = 9, fig.height=4.4, echo=FALSE, purl=FALSE}
```

Помимо всего прочего, что-то нужно делать с выбросами --- какие-то наблюдения потенциально могут значительно влиять на регрессию. 

---

## Какие наблюдения влияют на ход регрессии больше других?

.pull-left[
Влиятельные наблюдения, выбросы, outliers

- большая абсолютная величина остатка
- близость к краям области определения (leverage - рычаг, сила; иногда называют hat)

На графике точки и линии регрессии построенные с их включением:

- 1 - не влияет на ход регрессии, т.к. лежит на прямой
- 2 - умеренно влияет (большой остаток, малая сила влияния)
- 3 - очень сильно влияет (большой остаток, большая сила влияния)
]

.pull-right[
![](images/influential-observations.png)

.tiny[Из кн. Quinn, Keough, 2002, стр. 96, рис. 5.8] 
]

---

## Как оценить влиятельность наблюдений?

.pull-left[

### Расстояние Кука (Cook's d, Cook, 1977)

- Учитывает одновременно величину остатка и близость к краям области определения (leverage)
- Условное пороговое значение: выброс, если $d \ge 4/(n - p)$, где $n$ - объем выборки, $p$ - число параметров модели. Иногда используют более мягкий порог $d \ge 1$
]

.pull-right[
![](images/influential-observations.png)

.tiny[
Из кн. Quinn, Keough, 2002, стр. 96, рис. 5.8]

]

--

- Дж. Фокс советует не обращать внимания на пороговые значения (Fox, 1991)

---

## Вычисление расстояния Кука

описывает, как повлияет на модель удаление данного наблюдения

$$D_i = \frac{\sum{( \color{blue}{\hat{y_{j}}} - \color{red}{\hat{y}_{j(i)}})^2}}{p \; MS_e}$$

- $\color{blue}{\hat{y_j}}$ — значение предсказанное полной моделью
- $\color{red}{\hat{y}_{j(i)}}$ — значение, предсказанное моделью, построенной без учета $i$-го значения предиктора
- $p$ — количество параметров в модели
- $MS_{e}$ — среднеквадратичная ошибка модели ( $\hat\sigma^2$ )

---

## Что делать с влиятельными точками и с выбросами?

.pull-left[

- Проверить, не ошибка ли это. Если нет, не удалять - обсуждать!
- Проверить, что будет, если их исключить из модели
- Можно трансформировать данные! но тоже не всегда решает
]

.pull-right[
![](images/influential-observations.png)

.tiny[Из кн. Quinn, Keough, 2002, стр. 96, рис. 5.8] 
]

---

## Некоторые виды трансформаций

Трансформация  |  Формула  
------------- | -------------   
степень -2 | $1/x^2$
степень -1 | $1/x$
степень -0.5  | $1/\sqrt{x}$
степень 0.5 | $\sqrt{x}$
логарифмирование | $log(x)$  

---

class: middle, center, inverse

# Проверка условий применимости линейной регрессии в R

---

## Как проверить условия применимости?

1. Вычисляем VIF --- коллинеарность предикторов (для множественной регрессии с этого всегда нужно начинать);
2. График расстояния Кука для разных наблюдений --- проверка на наличие выбросов;
3. График остатков от предсказанных значений --- величина остатков, влиятельность наблюдений, отсутствие паттернов, гомогенность дисперсий;
4. График квантилей остатков --- распределение остатков.

---

## 1. Проверка на коллинеарность предикторов

```{r message = FALSE}
library(car)
vif(river_lm1) # variance inflation factors
```

--

Самое большое значение vif для предиктора `Forest`. Удалим его из модели и пересчитаем vif.

--

```{r}
river_lm2 <- lm(Nitrogen ~ Agr + Rsdntial + ComIndl, data = river)
vif(river_lm2) # variance inflation factors
```

--

Самое большое значение vif для предиктора `ComIndl`. Аналогично.

---

## 1. Проверка на коллинеарность предикторов, продолжение

Удаляем `ComIndl`

```{r}
river_lm3 <- lm(Nitrogen ~ Agr + Rsdntial, data = river)
vif(river_lm3) # variance inflation factors
```

--

Все в порядке. Судя по значениям vif после пошагового удаления всех коллинеарных предикторов оставшиеся предикторы независимы. 

Теперь наша модель `river_lm3` выглядит так:

$Nitrogen_i = b_0 + b_1 Agr_i + b_3 Rsdntial_i + e_i$

---

## Для анализа остатков создадим диангостический датафрейм

```{r}
library(ggplot2) # там есть функция fortify()
river_diag3 <- fortify(river_lm3)
# вот, что записано в диагностическом датафрейме
head(river_diag3, 2)
```

- `.hat` — "сила воздействия" данного наблюдения (leverage)
- `.cooksd` - расстояние Кука  
- `.fitted` - предсказанные значения  
- `.resid` - остатки  
- `.stdresid` - стандартизованные остатки

---

## 2. Проверка на наличие влиятельных наблюдений

График расстояния Кука для всех наблюдений

```{r}
ggplot(data = river_diag3, aes(x = 1:nrow(river_diag3), y = .cooksd)) + 
  geom_bar(stat = "identity")
```

--

Вот оно, то самое отскакивающее значение `Rsdntial` больше 25% застройки. Сейчас оно слишком сильно влияет на ход регрессии. Давайте попробуем его удалить и переподобрать модель.

---

## Новая модель, на очищенных данных

```{r fig.height=2.25}
# данные без выброса
river_subset <- river[river$Rsdntial < 25, ]
# новая модель
river_lm4 <- lm(Nitrogen ~ Agr + Rsdntial, data = river_subset)
# диагностический датафрейм
river_diag4 <- fortify(river_lm4)
# график расстояния Кука
ggplot(data = river_diag4, aes(x = 1:nrow(river_diag4), y = .cooksd)) + 
  geom_bar(stat = "identity")
```

--

Отлично, больше нет чрезмерно влиятельных наблюдений с $d > 1$.

---

## Задача

Постройте график зависимости стандартизованных остатков от предсказанных значений

Используйте данные из `river_diag4`

```{r resid-plot, purl=FALSE, echo=FALSE}
gg_resid <- ggplot(data = river_diag4, aes(x = .fitted, y = .stdresid)) + 
  geom_point()
gg_resid
```

---

## 3. График зависимости стандартизованных остатков от предсказанных значений

```{r resid-plot, purl=FALSE, echo=TRUE}
```

--

Большая часть стандартизованных остатков в пределах двух стандартных отклонений. В правой части графика мало наблюдений (с большими предсказанными значениями концентрации азота) - с этим ничего не поделаешь... Тренда среди остатков нет.

---

## 4. Квантильный график стандартизованных остатков

Используется, чтобы оценить форму распределения. По оси Х --- квантили теоретического распределения, по оси Y --- квантили остатков модели.

Если точки лежат на одной прямой --- все в порядке.

```{r qqplot, warning = FALSE, message=FALSE, fig.width = 6, fig.height=5, echo=-c(1, 4)}
op <- par(cex = 0.8, mar = c(4, 4, 1, 1))
library(car)
qqPlot(river_lm4, id = FALSE) # из пакета car
par(op)
```

---

## Интерпретируем квантильный график 

Какие выводы можно сделать по квантильному графику?

```{r qqplot, warning = FALSE, message=FALSE, echo=FALSE, purl=FALSE, fig.width = 6}
```

--

Отклонений от нормального распределения нет.

### Внимание!

Только если все условия выполняются, можно приступить к интерпретации результатов тестов значимости коэффициентов регрессии.

---

## Интерпретация коэффициентов регрессии

```{r}
coef(river_lm4)
```

--

### Обычные коэффициенты

- Величина обычных коэффициентов зависит от единиц измерения
- $b_0$ --- Отрезок (Intercept), отсекаемый регрессионной прямой на оси $y$. Значение зависимой переменной $Y$, если предикторы равны нулю.
- Коэффициенты при предикторах показывают, на сколько изменяется $Y$, когда данный предиктор меняется на единицу, при условии, что остальные предикторы не меняют своих значений.

---

## Если предикторы измерены в разных единицах

Обычные коэффициенты отражают силу влияния предикторов, но не учитывают масштаб их варьирования.

Если стандартизовать переменные ( $x_{std} = \frac{x_i - \bar x}{SD_x}$), то масштабы их изменений выровняются: они будут измеряться в одних и тех же единицах — в стандартных отклонениях.

```{r purl=FALSE, echo=FALSE}
op <- par(mfrow = c(1, 2), mar = c(2, 3, 3, 0.3))
boxplot(river_subset[, c("Agr", "Rsdntial")], main = "Исходно")
boxplot(scale(river_subset[, c("Agr", "Rsdntial")]), main = "После\n стандартизации")
par(op)
```

--

Если подобрать по линейную регрессию по стандартизованным значениям предикторов, то можно будет сравнивать силу их влияния с учетом масштаба их варьирования.

---

## Для сравнения влияния разных предикторов --- стандартизованные коэффициенты

```{r}
scaled_river_lm4 <- lm(Nitrogen ~ scale(Agr) + scale(Rsdntial), 
                       data = river_subset)
coef(scaled_river_lm4)
```

--

### Стандартизованные коэффициенты

> - Стандартизованные коэффициенты измерены в стандартных отклонениях. Их можно сравнивать друг с другом, поскольку они дают относительную оценку влияния фактора.
>- $b_0$ --- Отрезок (Intercept), отсекаемый регрессионной прямой на оси $y$. Значение зависимой переменной $Y$, если предикторы равны нулю. Для стандартизованных величин среднее значение равно нулю, поэтому $b_0$ --- это значение зависимой переменной при средних значениях всех предикторов.
>- Коэффициенты при предикторах показывают, на сколько изменяется $Y$, когда предиктор меняется на одно стандартное отклонение, при условии, что остальные предикторы не меняют своих значений. Это относительная оценка влияния фактора.

---

## Задача

Определите по значениям стандартизованных коэффициентов, какие предикторы сильнее всего влияют на концентрацию азота в воде?

```{r}
summary(scaled_river_lm4)
```

--

Влияние обоих предикторов сопоставимо по силе, но сильнее всего все же влияет процент застройки `Rsdntial`.

---

## Оценка качества подгонки модели

```{r}
summary(river_lm4)$adj.r.squared
```

### Обычный $R^2$ --- доля объясненной изменчивости

$$R^2 =\frac{SS_{r}}{SS_{t}} = 1 - \frac{SS_{e}}{SS_{t}}$$

__Не используйте обычный $R^2$ для множественной регрессии!__

--

### $R^2_{adj}$ --- cкорректированный $R^2$

$$R^2_{adj} = 1 - (1 - R^2) \frac{n - 1}{n - p}$$

где $n - p = df_{e}$, $n - 1 = df_{t}$

$R^2_{adj}$ учитывает число переменных в модели, вводится штраф за каждый новый параметр.

Используйте $R^2 _{adj}$ для сравнения моделей с разным числом параметров.

---

## Описание результатов

Для описания зависимости концентрации азота в речной воде от особенностей землепользования была подобрана линейная модель: $`r lm_equation(river_lm4, strict=FALSE, digits = 1)`$, где $Agr$ — процент сельскохозяйственных земель, $Rsdntial$ — процент земель, занятых поселениями. Эта модель объяснила `r round(summary(river_lm4)$adj.r.squared * 100, 1)`% общей изменчивости концентрации азота в речной воде. С увеличением процента застройки и процента сельскохозяйственных земель в бассейнах рек концентрация азота статистически значимо увеличивалась. 

| | Оценка | Ст.ошибка | t | P | 
| ----| ------- | --------- | -- | ----| 
| Отрезок | 0.52 | 0.078 | 6.71 | < 0.01 | 
| Agr | 0.01 | 0.003 | 5.40 | < 0.01 | 
| Rsdntial | 0.22 | 0.032 | 6.92 | < 0.01 | 


```{r echo=FALSE, results='hide', purl=FALSE}
library(xtable)
library(tidyr)
smr <- coef(summary(river_lm4)) %>% data.frame()
# %>% format.data.frame(., digits = 2, nsmall = 1)
smr$Pr...t.. <- format.pval(smr$Pr...t.., eps = 0.01)
rownames(smr)[1] <- "Отрезок"
colnames(smr) <- c("Оценка", "Ст.ошибка", "t", "P")

xtb <- xtable(
  smr,
  caption = "Коэффициенты линейной регрессии, описывающей зависимость средней концентрации азота в воде (мг/л) от характеристик землепользования: Agr — процент сельскохозяйственных земель, Rsdntial — процент земель, занятых поселениями. t --- значение t-критерия, P --- уровень значимости.", digits = c(0, 2, 3, 2, 2),
  label = "tab:mreg-coef")

print.xtable(xtb, comment = F, caption.placement = "top")
```

---

## Take-home messages

- Для сравнения влияния разных предикторов можно использовать бета-коэффициенты
- Условия применимости линейной регрессии должны выполняться, чтобы можно было тестировать гипотезы
    1. Независимость
    1. Линейность 
    1. Нормальное распределение
    1. Гомогенность дисперсий
    1. Отсутствие коллинеарности предикторов (для множественной регрессии в первую очередь!)

---

## Дополнительные ресурсы

+ Кабаков Р.И. R в действии. Анализ и визуализация данных на языке R. М.: ДМК Пресс, 2014
+ Diez, D.M., Barr, C.D. and Çetinkaya-Rundel, M., 2015. OpenIntro Statistics. OpenIntro.
+ Zuur, A., Ieno, E.N. and Smith, G.M., 2007. Analyzing ecological data. Springer Science & Business Media.
+ Quinn G.P., Keough M.J. 2002. Experimental design and data analysis for biologists
+ Logan M. 2010. Biostatistical Design and Analysis Using R. A Practical Guide
+ Видосики на ютубе от [StatQuest](https://www.youtube.com/@statquest)
